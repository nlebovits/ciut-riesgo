---
title: "RENABAP"
subtitle: "Análisis de la exposición poblacional a peligros de inundación en el Partido de La Plata"
---

## Resumen ejecutivo

## Objetivos

El objetivo principal de este análisis es calcular con mayor precisión la exposición poblacional en la región del Partido de La Plata, comparando el enfoque actual de interpolación por área de datos del RENABAP con enfoques de mapeo dasimétrico de mayor resolución utilizando datos de la Capa Global de Asentamientos Humanos (GHSL) y el conjunto de datos de edificios abiertos de Google-Microsoft OpenStreetMap.

Para lograr este objetivo, necesitamos:

1. **Explicar y comparar metodologías**: Desarrollar una comprensión clara de las diferencias entre interpolación por área y mapeo dasimétrico
2. **Evaluar fuentes de datos**: Analizar las ventajas y limitaciones de cada conjunto de datos utilizado
3. **Crear estimaciones robustas**: Generar un rango de posibles exposiciones poblacionales para informar la toma de decisiones
4. **Priorizar recursos**: Identificar áreas donde se requiera recopilación de datos más precisa

## Contexto

## Fuentes de datos

```{python}


import matplotlib.pyplot as plt
import contextily as ctx


from io import BytesIO, StringIO
from owslib.wfs import WebFeatureService
from shapely.geometry import box
import geopandas as gpd
import requests
import pandas as pd
import pandas as pd
import geopandas as gpd
import requests
from io import StringIO

import boto3
import duckdb


import matplotlib.pyplot as plt

import numpy as np
import s2sphere
from botocore.config import Config
from rasterstats import zonal_stats
import matplotlib.pyplot as plt
import rasterstats
import matplotlib.pyplot as plt
from scipy import stats
import seaborn as sns
from rasterio.features import rasterize
import numpy as np


import rioxarray
from shapely.geometry import box
from shapely.geometry import box


USE_CRS = "EPSG:5349"


# Generic mapping functions
def setup_base_map(figsize=(12, 10), bounds=None, padding_x=500, padding_y=500):
    """Create figure and set up basic map boundaries with padding."""
    if bounds is None:
        bounds = renabap_pba_intersect.total_bounds

    # Convert bounds to Web Mercator for basemap compatibility
    if bounds is not None:
        # Create a temporary GeoDataFrame with the bounds to reproject
        temp_bounds = gpd.GeoDataFrame(
            geometry=[box(bounds[0], bounds[1], bounds[2], bounds[3])], crs=USE_CRS
        )
        bounds_3857 = temp_bounds.to_crs("EPSG:3857").total_bounds
    else:
        bounds_3857 = bounds

    fig, ax = plt.subplots(figsize=figsize)
    ax.set_xlim(bounds_3857[0] - padding_x, bounds_3857[2] + padding_x)
    ax.set_ylim(bounds_3857[1] - padding_y, bounds_3857[3] + padding_y)
    return fig, ax


def add_basemap(ax, zoom=13):
    """Add CartoDB basemap to the axes."""
    # The axes are already in Web Mercator from setup_base_map
    ctx.add_basemap(
        ax,
        source=ctx.providers.CartoDB.PositronNoLabels,
        zorder=0,
        zoom=zoom,
    )

    return ax


def add_north_arrow(ax, x=0.95, y=0.05, arrow_length=0.04):
    """Add a north arrow to the map."""
    # Add north arrow, https://stackoverflow.com/a/58110049/604456
    ax.annotate('N', xy=(x, y), xytext=(x, y-arrow_length),
                arrowprops=dict(facecolor='black', width=3, headwidth=10),
                ha='center', va='center', fontsize=14,
                xycoords=ax.transAxes)


def add_la_plata_outline(ax):
    """Add the outline of Partido de La Plata to a map."""
    la_plata_3857 = la_plata.to_crs("EPSG:3857")
    la_plata_3857.plot(
        ax=ax,
        facecolor='none',
        edgecolor='black',
        linewidth=0.5,
        linestyle='--',
        legend=False,
        zorder=5
    )

def create_consistent_map(title, bounds=None):
    """Create a map with consistent styling and basemap."""
    fig, ax = setup_base_map(bounds=bounds)

    # Add basemap
    add_basemap(ax)

    # Add north arrow
    add_north_arrow(ax)

    # Add La Plata partido outline
    add_la_plata_outline(ax)

    # Set title
    ax.set_title(title, fontsize=16, fontweight="bold", pad=20)

    # Remove axes
    ax.set_axis_off()

    return fig, ax



def wfs_to_gdf(
    wfs_url: str, layer_name: str, srs: str = "EPSG:4326"
) -> gpd.GeoDataFrame:
    """
    Descarga una capa WFS y la devuelve como GeoDataFrame.

    Args:
        wfs_url (str): URL del servicio WFS.
        layer_name (str): Nombre de la capa (typename).
        srs (str): Código EPSG del sistema de referencia de coordenadas.

    Returns:
        gpd.GeoDataFrame: Capa descargada como GeoDataFrame.
    """
    wfs = WebFeatureService(url=wfs_url, version="2.0.0")
    response = wfs.getfeature(typename=layer_name, srsname=srs)
    gdf = gpd.read_file(BytesIO(response.read()))
    return gdf



```

### RENABAP

El Registro Nacional de Barrios Populares (RENABAP) es producido por la Subsecretaría de Integración Socio Urbana y proporciona información sobre asentamientos informales en Argentina, incluyendo estimaciones de población y delimitaciones geográficas de estos barrios. Más información sobre el RENABAP está disponible en el [Observatorio de Barrios Populares](https://www.argentina.gob.ar/obras-publicas/sisu/renabap/observatorio-de-barrios-populares). Los datos fueron obtenidos a través del [Mapa de Barrios Populares](https://www.argentina.gob.ar/obras-publicas/sisu/renabap/mapa) y están disponibles para [descarga como GeoJSON](https://www.argentina.gob.ar/sites/default/files/renabap-2023-12-06.geojson).

```{python}

### import data

response = requests.get(
    "https://www.argentina.gob.ar/sites/default/files/renabap-2023-12-06.geojson"
)
renabap = gpd.read_file(StringIO(response.text))
renabap_pba = renabap[renabap["provincia"] == "Buenos Aires"]
renabap_pba = renabap_pba.to_crs(USE_CRS)

peligro_path = "/home/nissim/Documents/dev/fulbright/ciut-riesgo/notebooks/data/la_plata_pelig_2023_datos_originales.geojson"
peligro = gpd.read_file(peligro_path)
peligro = peligro.to_crs(USE_CRS)

peligro_bounds = peligro.total_bounds
peligro_bbox = box(*peligro_bounds)

partidos = wfs_to_gdf(
    wfs_url="https://geo.arba.gov.ar/geoserver/idera/wfs",
    layer_name="idera:Departamento",
    srs="EPSG:5347",
)

partidos = partidos.to_crs(USE_CRS)
la_plata = partidos[partidos["fna"] == "Partido de La Plata"]

# Obtener la geometría principal
main_geom = la_plata.geometry.iloc[0]

# Si es un MultiPolygon, mantener solo el polígono más grande (el partido principal)
# Esto elimina la pequeña isla que aparece en los datos
if main_geom.geom_type == 'MultiPolygon':
    # Obtener todos los polígonos y mantener el que tenga mayor área
    largest_polygon = max(main_geom.geoms, key=lambda p: p.area)
    la_plata.geometry.iloc[0] = largest_polygon

la_plata_bbox = la_plata.geometry.iloc[0]

renabap_pba_intersect = renabap_pba[
    renabap_pba.geometry.intersects(la_plata_bbox)
].copy()



# Get common bounds for all maps
common_bounds = la_plata.total_bounds

# Intersect settlements with hazard zones
settlement_hazard = gpd.overlay(renabap_pba_intersect, peligro, how="intersection")
```

```{python}
fig1, ax1 = create_consistent_map("Asentamientos RENABAP en La Plata", common_bounds)

# Add RENABAP settlements as black outline with no fill
# Reproject to Web Mercator to match the map's coordinate system
renabap_pba_intersect_3857 = renabap_pba_intersect.to_crs("EPSG:3857")

renabap_pba_intersect_3857.plot(
    ax=ax1,
    facecolor='none',
    edgecolor='black',
    linewidth=0.5,
    legend=False,
    zorder=10
)


plt.tight_layout()
plt.show()
```

### GHSL

La [Capa Global de Asentamientos Humanos (Global Human Settlement Layer)](https://human-settlement.emergency.copernicus.eu/ghs_pop2023.php) [@ghsl_pop2023a] es un conjunto de datos de resolución de 100 metros que proporciona estimaciones de población multitemporales (1975-2030) derivadas de datos censales y administrativos, informadas por la distribución y clasificación de áreas construidas. El GHSL ya tiene un uso científico establecido para mapear la exposición poblacional a peligros de inundación [@tellman2021]. Sin embargo, esta fuente presenta limitaciones importantes: estudios sobre modelado de riesgo de inundación con conjuntos de datos globales han demostrado que evaluar la exposición a esta escala de resolución puede llevar a sobreestimaciones de la exposición poblacional en zonas de peligro de inundación en comparación con datos de mayor resolución [@smith2019].

```{python}

import rioxarray
from shapely.geometry import box

# Load GHSL data with dask chunking for memory efficiency
ghsl = rioxarray.open_rasterio(
    "/home/nissim/Downloads/spatial/GHS_POP_E2025_GLOBE_R2023A_54009_100_V1_0_R14_C13/GHS_POP_E2025_GLOBE_R2023A_54009_100_V1_0_R14_C13.tif",
    chunks={"x": 1024, "y": 1024},  # Adjust chunk size based on your memory constraints
)

# Reproject to your target CRS with streaming
ghsl = ghsl.rio.reproject(dst_crs=USE_CRS)

# Clip GHSL data to ONLY the Partido de La Plata boundaries
# This will remove any GHSL data outside the partido
ghsl_clipped = ghsl.rio.clip(
    [la_plata.geometry.iloc[0]],  # Use the actual La Plata partido geometry
    from_disk=True,  # Process from disk to avoid loading entire dataset into memory
)

```

```{python}
# Import config for color palette
from config import PINK_PALETTE
from jenkspy import jenks_breaks

# Map 2: GHSL population data
fig2, ax2 = create_consistent_map("Datos de población GHSL", common_bounds)

# Create masked array to hide zero values (do this before jenks classification)
import numpy.ma as ma
ghsl_masked = ma.masked_where(ghsl_clipped.values[0] == 0, ghsl_clipped.values[0])

# Calculate jenks breaks for GHSL data (excluding nodata values and zeros)
ghsl_valid = (ghsl_clipped.values[0] != -200) & (ghsl_clipped.values[0] != 0)
ghsl_valid_data = ghsl_clipped.values[0][ghsl_valid]
jenks_breaks = jenks_breaks(ghsl_valid_data, n_classes=10)

# Use plasma colormap from matplotlib
import matplotlib.pyplot as plt
plasma_cmap = plt.cm.plasma

# Reproject GHSL data to Web Mercator to match the map's coordinate system
ghsl_clipped_3857 = ghsl_clipped.rio.reproject("EPSG:3857")

# Ensure the reprojected data covers the entire map area
# Instead of clipping, let's make sure the data extends beyond the map bounds
# This will prevent the "box-ish" appearance at the edges

# Create masked array for the reprojected and clipped data
# Mask out both zeros AND nodata values to make areas outside the clipped zone transparent
ghsl_masked_3857 = ma.masked_where(
    (ghsl_clipped_3857.values[0] == 0) | (ghsl_clipped_3857.values[0] == -200),
    ghsl_clipped_3857.values[0]
)

# Plot GHSL raster with jenks classification and zero masking
im = ax2.imshow(
    ghsl_masked_3857,
    extent=[ghsl_clipped_3857.x.min(), ghsl_clipped_3857.x.max(), 
            ghsl_clipped_3857.y.min(), ghsl_clipped_3857.y.max()],
    cmap=plasma_cmap,
    alpha=0.75,
    vmin=jenks_breaks[0],
    vmax=jenks_breaks[-1]
)

# Add colorbar legend at the bottom with better positioning
cbar = plt.colorbar(im, ax=ax2, orientation='horizontal', 
                    location='bottom', shrink=0.6, aspect=40, pad=0.15)
cbar.set_label('Población GHSL', fontsize=10)

plt.tight_layout()
plt.show()
```

### Google-Microsoft-OSM Open Buildings

Los datos de [Google-Microsoft-OSM Open Buildings - combined by VIDA](https://source.coop/repositories/vida/google-microsoft-osm-open-buildings/access) [@google_microsoft_osm_buildings] representan una forma más precisa de evaluar dónde se ubican los asentamientos humanos. Este conjunto de datos combina Google's V3 Open Buildings, Microsoft's GlobalMLFootprints, y OpenStreetMap building footprints, conteniendo más de 2.7 mil millones de huellas de edificios. Estos datos han sido [exitosamente aplicados a evaluaciones de riesgo de inundación por empresas globales de riesgo financiero como ICE](https://www.ice.com/insights/sustainable-finance/ice-climates-exposure-datasets-understanding-how-climate-risks-impact-infrastructure-and-communities), demostrando su utilidad para mapear la exposición climática a nivel de huella de edificio individual. Sin embargo, en ausencia de información sobre si los edificios son residenciales o tienen otros usos, y sin datos sobre el número total de unidades en el edificio y habitantes por edificio, solo podemos obtener estimaciones proporcionales aproximadas de dónde se ubican las personas, sin tener una comprensión precisa de quién vive realmente allí y cuántas personas.

```{python}

#| cache: false
#| #| output: false

def fetch_buildings(geodataframe, temp_file="buildings_filtered.parquet"):
    """Fetch building data for a given GeoDataFrame region"""

    # Get S2 cell and bounds
    center = geodataframe.to_crs("epsg:3857").union_all().centroid
    center_wgs84 = (
        gpd.GeoDataFrame(geometry=[center], crs="EPSG:3857")
        .to_crs(epsg=4326)
        .geometry.iloc[0]
    )
    cell = s2sphere.CellId.from_lat_lng(
        s2sphere.LatLng.from_degrees(center_wgs84.y, center_wgs84.x)
    ).parent(10)
    bounds = geodataframe.to_crs("epsg:4326").total_bounds

    # Find matching S2 partition
    s3 = boto3.client(
        "s3",
        endpoint_url="https://data.source.coop",
        aws_access_key_id="",
        aws_secret_access_key="",
        config=Config(s3={"addressing_style": "path"}),
    )

    partitions = {
        obj["Key"].split("/")[-1].replace(".parquet", "")
        for obj in s3.list_objects_v2(
            Bucket="vida",
            Prefix="google-microsoft-osm-open-buildings/geoparquet/by_country_s2/country_iso=ARG/",
        ).get("Contents", [])
    }

    parent_id = next(
        str(cell.parent(level).id())
        for level in range(10, 0, -1)
        if str(cell.parent(level).id()) in partitions
    )

    # Setup DuckDB and query
    con = duckdb.connect()
    for cmd in [
        "INSTALL spatial",
        "LOAD spatial",
        "INSTALL httpfs",
        "LOAD httpfs",
        "SET s3_region='us-east-1'",
        "SET s3_endpoint='data.source.coop'",
        "SET s3_use_ssl=true",
        "SET s3_url_style='path'",
    ]:
        con.execute(cmd)

    # Export and read back
    query = f"""
    COPY (SELECT * FROM 's3://vida/google-microsoft-osm-open-buildings/geoparquet/by_country_s2/country_iso=ARG/{parent_id}.parquet'
          WHERE bbox.xmax >= {bounds[0]} AND bbox.xmin <= {bounds[2]} AND
                bbox.ymax >= {bounds[1]} AND bbox.ymin <= {bounds[3]}
    ) TO '{temp_file}' (FORMAT PARQUET);
    """

    con.execute(query)
    df = pd.read_parquet(temp_file)
    df["geometry"] = gpd.GeoSeries.from_wkb(df["geometry"])

    return gpd.GeoDataFrame(df, geometry="geometry", crs="EPSG:4326")


# Usage:
buildings = fetch_buildings(renabap_pba_intersect)

# Reproject buildings to match the analysis CRS
buildings_proj = buildings.to_crs(USE_CRS)

# clip the buildings to the partido de la plata
buildings_proj = buildings_proj.clip(la_plata)
```

```{python}
# Map 3: Building footprints
fig3, ax3 = create_consistent_map("Huellas de edificios", common_bounds)

# Add building footprints with grey fill and no outline
buildings_3857 = buildings_proj.to_crs("EPSG:3857")

buildings_3857.plot(
    ax=ax3,
    facecolor='grey',
    edgecolor='none',
    alpha=0.7
)

plt.tight_layout()
plt.show()
```

## Metodología y procesamiento

### Interpolación por area

La [interpolación areal](https://pysal.org/tobler/notebooks/01_interpolation_methods_overview.html) es un método simple en el que las variables de los datos fuente se ponderan según la superposición entre polígonos fuente y objetivo, luego se reagregan para ajustarse a las geometrías de los polígonos objetivo. En nuestro análisis, esto significa distribuir proporcionalmente la población de cada barrio popular según el área de intersección con diferentes niveles de peligro de inundación. El analasis original de la exposición poblacional a peligros de inundación en la región del Partido de La Plata se realizó utilizando este método.

```{python}
# Ensure both GeoDataFrames have the same CRS
if renabap_pba_intersect.crs != peligro.crs:
    peligro = peligro.to_crs(renabap_pba_intersect.crs)

# Get unique hazard levels
hazard_levels = peligro["PELIGROSID"].unique()

# Initialize result columns
renabap_with_porciones = renabap_pba_intersect.copy()
for level in hazard_levels:
    renabap_with_porciones[f"porcion_{level}"] = 0.0

# Calculate total area of each barrio
renabap_with_porciones['total_area'] = renabap_with_porciones.geometry.area

# For each barrio, calculate intersection with each hazard level
for idx, barrio in renabap_with_porciones.iterrows():
    barrio_geom = barrio.geometry
    barrio_total_area = barrio_geom.area
    
    if barrio_total_area == 0:
        continue
        
    for level in hazard_levels:
        hazard_subset = peligro[peligro["PELIGROSID"] == level]
        
        if hazard_subset.empty:
            continue
        
        # Calculate intersection area
        intersection_area = 0
        for _, hazard_row in hazard_subset.iterrows():
            try:
                intersection = barrio_geom.intersection(hazard_row.geometry)
                if not intersection.is_empty:
                    intersection_area += intersection.area
            except Exception as e:
                print(f"Error calculating intersection for {barrio.get('nombre_barrio', idx)}: {e}")
                continue
        
        # Calculate proportion
        proportion = intersection_area / barrio_total_area if barrio_total_area > 0 else 0
        renabap_with_porciones.at[idx, f"porcion_{level}"] = proportion

# Create tidy format dataframe
renabap_tidy = []

for idx, row in renabap_with_porciones.iterrows():
    for level in hazard_levels:
        familias_expuestas = row[f"porcion_{level}"] * row["familias_aproximadas"]
        
        renabap_tidy.append({
            "id_renabap": row["id_renabap"],
            "nombre_barrio": row["nombre_barrio"],
            "peligrosidad": level,
            "fam_expuestas_areal": familias_expuestas
        })

renabap_tidy = pd.DataFrame(renabap_tidy)
```

### Cuenta de edificios

### Mapeo dasymetrico

El [mapeo dasimétrico](https://support.esri.com/en-us/gis-dictionary/dasymetric-mapping) reorganiza datos cartográficos de una unidad de recolección en áreas más precisas, modificando los límites originales usando datos de apoyo relacionados. Por ejemplo, un atributo de población organizado por tracto censal se vuelve más significativo cuando se eliminan áreas donde es razonable inferir que la gente no vive (cuerpos de agua, terrenos vacíos). En nuestro caso, utilizamos datos GHSL y huellas de edificios como información auxiliar para mejorar la precisión de las estimaciones de distribución poblacional.

#### Mapeo dasymetrico con datos de edificios

```{python}
# Step 1: Calculate buildings per settlement-hazard intersection
buildings_hazard = gpd.overlay(buildings_proj, settlement_hazard, how="intersection")

# Count buildings per settlement-hazard combination
buildings_per_hazard = (
    buildings_hazard.groupby(["id_renabap", "PELIGROSID"])
    .size()
    .reset_index(name="buildings_count")
)

# Step 2: Calculate total buildings per settlement (barrio popular)
buildings_settlement = gpd.overlay(
    buildings_proj, renabap_pba_intersect, how="intersection"
)
total_buildings_per_settlement = (
    buildings_settlement.groupby("id_renabap")
    .size()
    .reset_index(name="total_buildings")
)

# Step 3: Merge and calculate ratios
hazard_ratios = buildings_per_hazard.merge(
    total_buildings_per_settlement, on="id_renabap", how="left"
)
hazard_ratios["building_ratio"] = (
    hazard_ratios["buildings_count"] / hazard_ratios["total_buildings"]
)

# Step 4: Get total population per settlement and apply ratios
settlement_population = renabap_pba_intersect[
    ["id_renabap", "familias_aproximadas"]
].copy()

# Merge with ratios and calculate population estimates
population_estimates = hazard_ratios.merge(
    settlement_population, on="id_renabap", how="left"
)
population_estimates["estimated_population_hazard"] = (
    population_estimates["building_ratio"]
    * population_estimates["familias_aproximadas"]
)

# Step 5: Create final results with totals
final_results = population_estimates[
    ["id_renabap", "PELIGROSID", "estimated_population_hazard"]
].copy()

# Add total population rows (no hazard breakdown)
total_pop_rows = settlement_population.copy()
total_pop_rows["PELIGROSID"] = "total"
total_pop_rows["estimated_population_hazard"] = total_pop_rows["familias_aproximadas"]

# Combine
final_results = pd.concat(
    [
        final_results,
        total_pop_rows[["id_renabap", "PELIGROSID", "estimated_population_hazard"]],
    ],
    ignore_index=True,
)

# Create buildings tidy dataframe with matching structure
buildings_tidy = final_results[
    ["id_renabap", "PELIGROSID", "estimated_population_hazard"]
].copy()

# Rename columns to match the structure
buildings_tidy = buildings_tidy.rename(
    columns={
        "PELIGROSID": "peligrosidad",
        "estimated_population_hazard": "fam_expuestas_edificios",
    }
)

# Filter out the 'total' rows since we only want hazard-specific data
buildings_tidy = buildings_tidy[buildings_tidy["peligrosidad"] != "total"].copy()
```

#### Mapeo dasymetrico con datos GHSL

```{python}

import rasterstats

# Step 1: Calculate the total GHSL population per barrio popular using zonal statistics

# Convert to the format expected by rasterstats
geometries = [geom for geom in renabap_pba_intersect.geometry]

# Use rasterstats for vectorized zonal statistics
stats = rasterstats.zonal_stats(
    geometries,
    ghsl_clipped.values[0],  # rasterstats expects 2D array
    affine=ghsl_clipped.rio.transform(),
    stats=["sum"],
    nodata=ghsl_clipped.rio.nodata,
)

# Extract the sum values
ghsl_totals = [stat["sum"] if stat["sum"] is not None else 0 for stat in stats]

# Add the GHSL population estimates as a new column
renabap_pba_intersect["ghsl_pop_est"] = ghsl_totals

from rasterio.features import rasterize
import numpy as np

# Get the reference raster properties from GHSL data
reference_raster = ghsl_clipped
reference_transform = reference_raster.rio.transform()
reference_crs = reference_raster.rio.crs
reference_shape = reference_raster.shape[1:]  # Get 2D shape (height, width)


# Prepare geometries and values for rasterization
geometries_ghsl = [
    (geom, value)
    for geom, value in zip(
        renabap_pba_intersect.geometry, renabap_pba_intersect["ghsl_pop_est"]
    )
]
geometries_familias = [
    (geom, value)
    for geom, value in zip(
        renabap_pba_intersect.geometry, renabap_pba_intersect["familias_aproximadas"]
    )
]

# Create GHSL population raster
ghsl_pop_raster = rasterize(
    geometries_ghsl,
    out_shape=reference_shape,
    transform=reference_transform,
    fill=0,
    dtype=np.float32,
    all_touched=False,
)

# Create familias aproximadas raster
familias_raster = rasterize(
    geometries_familias,
    out_shape=reference_shape,
    transform=reference_transform,
    fill=0,
    dtype=np.float32,
    all_touched=False,
)


# Step 1: Divide original GHSL by the barrio-level GHSL to get fractional population
# Use masking to avoid division on invalid cells
mask = (ghsl_clipped.values[0] != -200) & (ghsl_pop_raster > 0.1)
ghsl_fractional = np.full_like(ghsl_clipped.values[0], -200, dtype=np.float64)
ghsl_fractional[mask] = ghsl_clipped.values[0][mask] / ghsl_pop_raster[mask]

# Step 2: Multiply fractional population by familias aproximadas to get downscaled data
mask2 = (ghsl_fractional != -200) & (familias_raster > 0)
familias_downscaled = np.full_like(ghsl_clipped.values[0], -200, dtype=np.float64)
familias_downscaled[mask2] = ghsl_fractional[mask2] * familias_raster[mask2]

# Verify the results - exclude -200 from range calculations
ghsl_valid = ghsl_clipped.values[0] != -200
fractional_valid = ghsl_fractional != -200
downscaled_valid = familias_downscaled != -200



# Check that the sum of downscaled familias equals the original familias aproximadas
total_original_familias = renabap_pba_intersect["familias_aproximadas"].sum()
total_downscaled_familias = np.sum(familias_downscaled[downscaled_valid])

# Create GHSL tidy dataframe with matching structure
ghsl_tidy = []

for idx, row in settlement_hazard.iterrows():
    stats = zonal_stats(
        [row.geometry],
        familias_downscaled,  # your numpy array
        affine=reference_transform,  # get transform from your xarray
        stats=["sum"],
        nodata=-200,  # use your actual nodata value
    )[0]

    ghsl_tidy.append(
        {
            "id_renabap": row["id_renabap"],
            "peligrosidad": row["PELIGROSID"],
            "fam_expuestas_ghsl": stats["sum"] if stats["sum"] is not None else 0,
        }
    )

ghsl_tidy = pd.DataFrame(ghsl_tidy)
```

## Resultados y conclusiones

### Comparación de métodos

### Exposición por barrio

```{python}
# Join all three dataframes by id_renabap and peligrosidad
final_df = renabap_tidy.merge(
    ghsl_tidy, on=["id_renabap", "peligrosidad"], how="outer"
)
final_df = final_df.merge(
    buildings_tidy, on=["id_renabap", "peligrosidad"], how="outer"
)

# Impute 0s for NA values in fam_expuestas columns
fam_expuestas_columns = [col for col in final_df.columns if 'fam_expuestas' in col]
final_df[fam_expuestas_columns] = final_df[fam_expuestas_columns].fillna(0)

# Get the geometry and nombre_barrio information from renabap_pba_intersect
# We only need one row per id_renabap since these are constant
renabap_info = renabap_pba_intersect[['id_renabap', 'nombre_barrio', 'geometry']].drop_duplicates(subset=['id_renabap'])

# Create long format dataframe with aggregation
final_tidy = []

# Add renabap data
for _, row in renabap_tidy.iterrows():
    # Get the corresponding barrio info
    barrio_info = renabap_info[renabap_info['id_renabap'] == row["id_renabap"]]
    if len(barrio_info) > 0:
        final_tidy.append(
            {
                "id_renabap": row["id_renabap"],
                "nombre_barrio": barrio_info.iloc[0]["nombre_barrio"],
                "geometry": barrio_info.iloc[0]["geometry"],
                "peligrosidad": row["peligrosidad"],
                "metodo": "area",
                "fam_expuestas": row["fam_expuestas_areal"],
            }
        )

# Add ghsl data
for _, row in ghsl_tidy.iterrows():
    barrio_info = renabap_info[renabap_info['id_renabap'] == row["id_renabap"]]
    if len(barrio_info) > 0:
        final_tidy.append(
            {
                "id_renabap": row["id_renabap"],
                "nombre_barrio": barrio_info.iloc[0]["nombre_barrio"],
                "geometry": barrio_info.iloc[0]["geometry"],
                "peligrosidad": row["peligrosidad"],
                "metodo": "ghsl",
                "fam_expuestas": row["fam_expuestas_ghsl"],
            }
        )

# Add buildings data
for _, row in buildings_tidy.iterrows():
    barrio_info = renabap_info[renabap_info['id_renabap'] == row["id_renabap"]]
    if len(barrio_info) > 0:
        final_tidy.append(
            {
                "id_renabap": row["id_renabap"],
                "nombre_barrio": barrio_info.iloc[0]["nombre_barrio"],
                "geometry": barrio_info.iloc[0]["geometry"],
                "peligrosidad": row["peligrosidad"],
                "metodo": "edificios",
                "fam_expuestas": row["fam_expuestas_edificios"],
            }
        )

final_tidy = pd.DataFrame(final_tidy)

# Aggregate to get one observation per barrio per hazard level per method
# Exclude geometry from groupby since it can't be sorted
final_tidy = (
    final_tidy.groupby(["id_renabap", "nombre_barrio", "peligrosidad", "metodo"])["fam_expuestas"]
    .sum()
    .reset_index()
)

# Create complete combination of all barrios, hazard levels, and methods
all_barrios = final_tidy["id_renabap"].unique()
all_hazard_levels = ["alta", "baja", "media"]
all_methods = ["area", "ghsl", "edificios"]

# Get the barrio info for complete combinations
complete_combinations = []
for barrio in all_barrios:
    barrio_info = renabap_info[renabap_info['id_renabap'] == barrio]
    if len(barrio_info) > 0:
        for hazard in all_hazard_levels:
            for method in all_methods:
                complete_combinations.append({
                    "id_renabap": barrio,
                    "nombre_barrio": barrio_info.iloc[0]["nombre_barrio"],
                    "peligrosidad": hazard,
                    "metodo": method
                })

complete_combinations = pd.DataFrame(complete_combinations)

# Merge with actual data and fill missing values with 0
final_tidy = complete_combinations.merge(
    final_tidy, on=["id_renabap", "nombre_barrio", "peligrosidad", "metodo"], how="left"
)
final_tidy["fam_expuestas"] = final_tidy["fam_expuestas"].fillna(0)

# Add geometry back to complete combinations - only once!
final_tidy = final_tidy.merge(
    renabap_info[['id_renabap', 'geometry']], 
    on='id_renabap', 
    how='left'
)

# Calculate total exposure per hazard level per method
summary = (
    final_tidy.groupby(["peligrosidad", "metodo"])["fam_expuestas"]
    .sum()
    .reset_index()
    .pivot(index="peligrosidad", columns="metodo", values="fam_expuestas")
)
```

```{python}
import matplotlib.pyplot as plt
import seaborn as sns

# Import categorical colors from config
from config import CATEGORICAL_COLORS

# Filter for high exposure (alta peligrosidad) using the joined dataframe
alta_data = final_tidy[final_tidy["peligrosidad"] == "alta"].copy()

# Aggregate by nombre_barrio and sum fam_expuestas for each method
# This handles cases where there are multiple geometries with the same barrio name
alta_aggregated = (
    alta_data.groupby(["nombre_barrio", "metodo"])["fam_expuestas"]
    .sum()
    .reset_index()
)

# Remove cases where the barrio name is "Sin Nombre"
alta_aggregated = alta_aggregated[alta_aggregated["nombre_barrio"] != "Sin Nombre"].copy()

# Calculate total exposure per barrio across all methods
total_exposure = (
    alta_aggregated.groupby("nombre_barrio")["fam_expuestas"]
    .sum()
    .sort_values(ascending=False)
)
top_10_barrios = total_exposure.head(10).index

# Filter aggregated data for top 10 barrios
top_10_data = alta_aggregated[
    alta_aggregated["nombre_barrio"].isin(top_10_barrios)
].copy()

# Create range plot showing min, max, and individual points
plt.figure(figsize=(14, 10))  # Increased height to accommodate longer barrio names

# Define colors for methods using categorical color scheme
method_colors = {
    "area": CATEGORICAL_COLORS[0],      # Blue alta
    "ghsl": CATEGORICAL_COLORS[1],      # Pink alta  
    "edificios": CATEGORICAL_COLORS[2]  # Teal
}

for i, barrio in enumerate(top_10_barrios):
    barrio_data = top_10_data[top_10_data["nombre_barrio"] == barrio]
    if len(barrio_data) > 0:
        values = barrio_data["fam_expuestas"].values
        min_val = values.min()
        max_val = values.max()

        # Plot range line
        plt.plot([min_val, max_val], [i, i], "k-", alpha=0.5, linewidth=2)

        # Plot individual points colored by method
        for _, row in barrio_data.iterrows():
            color = method_colors[row["metodo"]]
            plt.plot(row["fam_expuestas"], i, "o", color=color, markersize=6, alpha=0.8)

plt.yticks(range(len(top_10_barrios)), top_10_barrios)
plt.xlabel("Familias Expuestas")
plt.ylabel("Barrio")
plt.title("Range of High Exposure Estimates for Top 10 Barrios", fontsize=14)
plt.grid(True, alpha=0.3)

# Add legend
legend_elements = [
    plt.Line2D(
        [0],
        [0],
        marker="o",
        color="w",
        markerfacecolor=color,
        markersize=8,
        label=method,
    )
    for method, color in method_colors.items()
]
plt.legend(handles=legend_elements, title="Método")

plt.tight_layout()
plt.show()
```

```{python}
# create a dataframe that has all of the aggregated barrios, with the following columns:
    # nombre_barrio
    # peligrosidad
    # fam expouestas area
    # fam expouestas ghsl
    # fam expouestas edificios

# output a basic, scrollable table of the dataframe with simple formatting, including row highlighting, scrolling, and pagination, with 20 records per page

# Create aggregated dataframe with all barrios and methods
# First, create separate dataframes for each method
area_data = final_tidy[final_tidy["metodo"] == "area"].groupby(["nombre_barrio", "peligrosidad"])["fam_expuestas"].sum().reset_index()
area_data = area_data.rename(columns={"fam_expuestas": "fam_expuestas_area"})

ghsl_data = final_tidy[final_tidy["metodo"] == "ghsl"].groupby(["nombre_barrio", "peligrosidad"])["fam_expuestas"].sum().reset_index()
ghsl_data = ghsl_data.rename(columns={"fam_expuestas": "fam_expuestas_ghsl"})

edificios_data = final_tidy[final_tidy["metodo"] == "edificios"].groupby(["nombre_barrio", "peligrosidad"])["fam_expuestas"].sum().reset_index()
edificios_data = edificios_data.rename(columns={"fam_expuestas": "fam_expuestas_edificios"})

# Merge all methods together
barrio_summary = area_data.merge(ghsl_data, on=["nombre_barrio", "peligrosidad"], how="outer")
barrio_summary = barrio_summary.merge(edificios_data, on=["nombre_barrio", "peligrosidad"], how="outer")

# Fill NaN values with 0
barrio_summary = barrio_summary.fillna(0)

# Columns are already properly named from the aggregation

from itables import show

# Sort by nombre_barrio and peligrosidad in descending order
barrio_summary = barrio_summary.sort_values(["nombre_barrio", "peligrosidad"], ascending=True)

# Display as interactive table with pagination
show(barrio_summary)
```

This map is based on the "edificios" method, but could be replicated for the "area" or "GHSL" methods just as easily.

```{python}
# Map 4: Barrios por población expuesta estimada
fig4, ax4 = create_consistent_map("Barrios por población expuesta estimada", common_bounds)

# Get edificios method data from final_tidy, filtering out low risk
edificios_data = final_tidy[
    (final_tidy["metodo"] == "edificios") & 
    (final_tidy["peligrosidad"].isin(["alta", "media"]))
].copy()

# Convert to GeoDataFrame and reproject geometries to EPSG:3857 for proper plotting
edificios_gdf = gpd.GeoDataFrame(edificios_data, geometry='geometry', crs='EPSG:5349')
edificios_gdf = edificios_gdf.clip(la_plata)
edificios_gdf_3857 = edificios_gdf.to_crs('EPSG:3857')

# Import matplotlib colormap using the correct syntax
import matplotlib.pyplot as plt
import matplotlib.cm as cm

# Get RdPu colormap using the correct syntax
rdpu_cmap = cm.RdPu

# Define colors for peligrosidad levels using RdPu colormap
peligrosidad_colors = {
    "alta": rdpu_cmap(0.8),    # Darker red-purple for high risk
    "media": rdpu_cmap(0.4)    # Lighter red-purple for medium risk
}

# Define plotting order - medium risk first (underneath), high risk last (on top)
plotting_order = ["media", "alta"]

# Plot centroids for each barrio with jittering, in controlled order
np.random.seed(42)  # For reproducible results

for peligrosidad in plotting_order:
    # Get data for this peligrosidad level
    level_data = edificios_gdf_3857[edificios_gdf_3857["peligrosidad"] == peligrosidad]
    
    for _, row in level_data.iterrows():
        # Calculate centroid from the reprojected geometry
        centroid = row["geometry"].centroid
        
        # Add jitter
        jitter_x = np.random.uniform(-200, 200)
        jitter_y = np.random.uniform(-200, 200)
        
        x_pos = centroid.x + jitter_x
        y_pos = centroid.y + jitter_y
        
        # Get color based on peligrosidad
        color = peligrosidad_colors[row["peligrosidad"]]
        
        # Size based on number of families exposed
        size = max(10, row["fam_expuestas"] * 2 + 10)
        
        # Plot the centroid as a circle with jitter - use color keyword instead of c
        ax4.scatter(
            x_pos, y_pos, 
            s=size, color=color,  # Changed from c=color to color=color
            alpha=0.9,
            edgecolors='white',
            linewidth=1.0
        )

# Add legend for peligrosidad levels
legend_elements = [
    plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=color, markersize=8, label=level.capitalize())
    for level, color in peligrosidad_colors.items()
]
ax4.legend(handles=legend_elements, title="Nivel de Peligrosidad", loc='upper right')

plt.tight_layout()
plt.show()
```

### Exposición por cuenca

### Exposición por eje
