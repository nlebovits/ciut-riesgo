[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CIUT Riesgo",
    "section": "",
    "text": "Preface\nThis is a Quarto book.\nTo learn more about Quarto books visit https://quarto.org/docs/books.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "exposicion.html",
    "href": "exposicion.html",
    "title": "1  Exposición",
    "section": "",
    "text": "1.1 Introducción a la Exposición\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Exposición</span>"
    ]
  },
  {
    "objectID": "exposicion.html#análisis-de-exposición",
    "href": "exposicion.html#análisis-de-exposición",
    "title": "1  Exposición",
    "section": "1.2 Análisis de Exposición",
    "text": "1.2 Análisis de Exposición\nDuis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Exposición</span>"
    ]
  },
  {
    "objectID": "exposicion.html#evaluación-de-riesgos",
    "href": "exposicion.html#evaluación-de-riesgos",
    "title": "1  Exposición",
    "section": "1.3 Evaluación de Riesgos",
    "text": "1.3 Evaluación de Riesgos\nSed ut perspiciatis unde omnis iste natus error sit voluptatem accusantium doloremque laudantium, totam rem aperiam, eaque ipsa quae ab illo inventore veritatis et quasi architecto beatae vitae dicta sunt explicabo.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Exposición</span>"
    ]
  },
  {
    "objectID": "suavizacion.html",
    "href": "suavizacion.html",
    "title": "2  Suavización",
    "section": "",
    "text": "2.1 Conceptos de Suavización\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Suavización</span>"
    ]
  },
  {
    "objectID": "suavizacion.html#métodos-de-suavización",
    "href": "suavizacion.html#métodos-de-suavización",
    "title": "2  Suavización",
    "section": "2.2 Métodos de Suavización",
    "text": "2.2 Métodos de Suavización\nDuis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Suavización</span>"
    ]
  },
  {
    "objectID": "suavizacion.html#aplicaciones-prácticas",
    "href": "suavizacion.html#aplicaciones-prácticas",
    "title": "2  Suavización",
    "section": "2.3 Aplicaciones Prácticas",
    "text": "2.3 Aplicaciones Prácticas\nSed ut perspiciatis unde omnis iste natus error sit voluptatem accusantium doloremque laudantium, totam rem aperiam, eaque ipsa quae ab illo inventore veritatis et quasi architecto beatae vitae dicta sunt explicabo.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Suavización</span>"
    ]
  },
  {
    "objectID": "renabap.html",
    "href": "renabap.html",
    "title": "3  RENABAP",
    "section": "",
    "text": "3.1 Resumen ejecutivo",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>RENABAP</span>"
    ]
  },
  {
    "objectID": "renabap.html#análisis-de-datos-renabap",
    "href": "renabap.html#análisis-de-datos-renabap",
    "title": "3  RENABAP",
    "section": "3.2 Análisis de Datos RENABAP",
    "text": "3.2 Análisis de Datos RENABAP\nDuis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>RENABAP</span>"
    ]
  },
  {
    "objectID": "renabap.html#resultados-y-conclusiones",
    "href": "renabap.html#resultados-y-conclusiones",
    "title": "3  RENABAP",
    "section": "3.6 Resultados y conclusiones",
    "text": "3.6 Resultados y conclusiones\n\n3.6.1 Comparación de métodos\n\n\n3.6.2 Exposición por barrio\n\n\nMostrar código\n# Join all three dataframes by id_renabap and peligrosidad\nfinal_df = renabap_tidy.merge(\n    ghsl_tidy, on=[\"id_renabap\", \"peligrosidad\"], how=\"outer\"\n)\nfinal_df = final_df.merge(\n    buildings_tidy, on=[\"id_renabap\", \"peligrosidad\"], how=\"outer\"\n)\n\n# Impute 0s for NA values in fam_expuestas columns\nfam_expuestas_columns = [col for col in final_df.columns if 'fam_expuestas' in col]\nfinal_df[fam_expuestas_columns] = final_df[fam_expuestas_columns].fillna(0)\n\n# Get the geometry and nombre_barrio information from renabap_pba_intersect\n# We only need one row per id_renabap since these are constant\nrenabap_info = renabap_pba_intersect[['id_renabap', 'nombre_barrio', 'geometry']].drop_duplicates(subset=['id_renabap'])\n\n# Create long format dataframe with aggregation\nfinal_tidy = []\n\n# Add renabap data\nfor _, row in renabap_tidy.iterrows():\n    # Get the corresponding barrio info\n    barrio_info = renabap_info[renabap_info['id_renabap'] == row[\"id_renabap\"]]\n    if len(barrio_info) &gt; 0:\n        final_tidy.append(\n            {\n                \"id_renabap\": row[\"id_renabap\"],\n                \"nombre_barrio\": barrio_info.iloc[0][\"nombre_barrio\"],\n                \"geometry\": barrio_info.iloc[0][\"geometry\"],\n                \"peligrosidad\": row[\"peligrosidad\"],\n                \"metodo\": \"area\",\n                \"fam_expuestas\": row[\"fam_expuestas_areal\"],\n            }\n        )\n\n# Add ghsl data\nfor _, row in ghsl_tidy.iterrows():\n    barrio_info = renabap_info[renabap_info['id_renabap'] == row[\"id_renabap\"]]\n    if len(barrio_info) &gt; 0:\n        final_tidy.append(\n            {\n                \"id_renabap\": row[\"id_renabap\"],\n                \"nombre_barrio\": barrio_info.iloc[0][\"nombre_barrio\"],\n                \"geometry\": barrio_info.iloc[0][\"geometry\"],\n                \"peligrosidad\": row[\"peligrosidad\"],\n                \"metodo\": \"ghsl\",\n                \"fam_expuestas\": row[\"fam_expuestas_ghsl\"],\n            }\n        )\n\n# Add buildings data\nfor _, row in buildings_tidy.iterrows():\n    barrio_info = renabap_info[renabap_info['id_renabap'] == row[\"id_renabap\"]]\n    if len(barrio_info) &gt; 0:\n        final_tidy.append(\n            {\n                \"id_renabap\": row[\"id_renabap\"],\n                \"nombre_barrio\": barrio_info.iloc[0][\"nombre_barrio\"],\n                \"geometry\": barrio_info.iloc[0][\"geometry\"],\n                \"peligrosidad\": row[\"peligrosidad\"],\n                \"metodo\": \"edificios\",\n                \"fam_expuestas\": row[\"fam_expuestas_edificios\"],\n            }\n        )\n\nfinal_tidy = pd.DataFrame(final_tidy)\n\n# Aggregate to get one observation per barrio per hazard level per method\n# Exclude geometry from groupby since it can't be sorted\nfinal_tidy = (\n    final_tidy.groupby([\"id_renabap\", \"nombre_barrio\", \"peligrosidad\", \"metodo\"])[\"fam_expuestas\"]\n    .sum()\n    .reset_index()\n)\n\n# Create complete combination of all barrios, hazard levels, and methods\nall_barrios = final_tidy[\"id_renabap\"].unique()\nall_hazard_levels = [\"alta\", \"baja\", \"media\"]\nall_methods = [\"area\", \"ghsl\", \"edificios\"]\n\n# Get the barrio info for complete combinations\ncomplete_combinations = []\nfor barrio in all_barrios:\n    barrio_info = renabap_info[renabap_info['id_renabap'] == barrio]\n    if len(barrio_info) &gt; 0:\n        for hazard in all_hazard_levels:\n            for method in all_methods:\n                complete_combinations.append({\n                    \"id_renabap\": barrio,\n                    \"nombre_barrio\": barrio_info.iloc[0][\"nombre_barrio\"],\n                    \"peligrosidad\": hazard,\n                    \"metodo\": method\n                })\n\ncomplete_combinations = pd.DataFrame(complete_combinations)\n\n# Merge with actual data and fill missing values with 0\nfinal_tidy = complete_combinations.merge(\n    final_tidy, on=[\"id_renabap\", \"nombre_barrio\", \"peligrosidad\", \"metodo\"], how=\"left\"\n)\nfinal_tidy[\"fam_expuestas\"] = final_tidy[\"fam_expuestas\"].fillna(0)\n\n# Add geometry back to complete combinations - only once!\nfinal_tidy = final_tidy.merge(\n    renabap_info[['id_renabap', 'geometry']], \n    on='id_renabap', \n    how='left'\n)\n\n# Calculate total exposure per hazard level per method\nsummary = (\n    final_tidy.groupby([\"peligrosidad\", \"metodo\"])[\"fam_expuestas\"]\n    .sum()\n    .reset_index()\n    .pivot(index=\"peligrosidad\", columns=\"metodo\", values=\"fam_expuestas\")\n)\n\n\n\n\nMostrar código\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Import categorical colors from config\nfrom config import CATEGORICAL_COLORS\n\n# Filter for high exposure (alta peligrosidad) using the joined dataframe\nalta_data = final_tidy[final_tidy[\"peligrosidad\"] == \"alta\"].copy()\n\n# Aggregate by nombre_barrio and sum fam_expuestas for each method\n# This handles cases where there are multiple geometries with the same barrio name\nalta_aggregated = (\n    alta_data.groupby([\"nombre_barrio\", \"metodo\"])[\"fam_expuestas\"]\n    .sum()\n    .reset_index()\n)\n\n# Remove cases where the barrio name is \"Sin Nombre\"\nalta_aggregated = alta_aggregated[alta_aggregated[\"nombre_barrio\"] != \"Sin Nombre\"].copy()\n\n# Calculate total exposure per barrio across all methods\ntotal_exposure = (\n    alta_aggregated.groupby(\"nombre_barrio\")[\"fam_expuestas\"]\n    .sum()\n    .sort_values(ascending=False)\n)\ntop_10_barrios = total_exposure.head(10).index\n\n# Filter aggregated data for top 10 barrios\ntop_10_data = alta_aggregated[\n    alta_aggregated[\"nombre_barrio\"].isin(top_10_barrios)\n].copy()\n\n# Create range plot showing min, max, and individual points\nplt.figure(figsize=(14, 10))  # Increased height to accommodate longer barrio names\n\n# Define colors for methods using categorical color scheme\nmethod_colors = {\n    \"area\": CATEGORICAL_COLORS[0],      # Blue alta\n    \"ghsl\": CATEGORICAL_COLORS[1],      # Pink alta  \n    \"edificios\": CATEGORICAL_COLORS[2]  # Teal\n}\n\nfor i, barrio in enumerate(top_10_barrios):\n    barrio_data = top_10_data[top_10_data[\"nombre_barrio\"] == barrio]\n    if len(barrio_data) &gt; 0:\n        values = barrio_data[\"fam_expuestas\"].values\n        min_val = values.min()\n        max_val = values.max()\n\n        # Plot range line\n        plt.plot([min_val, max_val], [i, i], \"k-\", alpha=0.5, linewidth=2)\n\n        # Plot individual points colored by method\n        for _, row in barrio_data.iterrows():\n            color = method_colors[row[\"metodo\"]]\n            plt.plot(row[\"fam_expuestas\"], i, \"o\", color=color, markersize=6, alpha=0.8)\n\nplt.yticks(range(len(top_10_barrios)), top_10_barrios)\nplt.xlabel(\"Familias Expuestas\")\nplt.ylabel(\"Barrio\")\nplt.title(\"Range of High Exposure Estimates for Top 10 Barrios\", fontsize=14)\nplt.grid(True, alpha=0.3)\n\n# Add legend\nlegend_elements = [\n    plt.Line2D(\n        [0],\n        [0],\n        marker=\"o\",\n        color=\"w\",\n        markerfacecolor=color,\n        markersize=8,\n        label=method,\n    )\n    for method, color in method_colors.items()\n]\nplt.legend(handles=legend_elements, title=\"Método\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nMostrar código\n# create a dataframe that has all of the aggregated barrios, with the following columns:\n    # nombre_barrio\n    # peligrosidad\n    # fam expouestas area\n    # fam expouestas ghsl\n    # fam expouestas edificios\n\n# output a basic, scrollable table of the dataframe with simple formatting, including row highlighting, scrolling, and pagination, with 20 records per page\n\n# Create aggregated dataframe with all barrios and methods\n# First, create separate dataframes for each method\narea_data = final_tidy[final_tidy[\"metodo\"] == \"area\"].groupby([\"nombre_barrio\", \"peligrosidad\"])[\"fam_expuestas\"].sum().reset_index()\narea_data = area_data.rename(columns={\"fam_expuestas\": \"fam_expuestas_area\"})\n\nghsl_data = final_tidy[final_tidy[\"metodo\"] == \"ghsl\"].groupby([\"nombre_barrio\", \"peligrosidad\"])[\"fam_expuestas\"].sum().reset_index()\nghsl_data = ghsl_data.rename(columns={\"fam_expuestas\": \"fam_expuestas_ghsl\"})\n\nedificios_data = final_tidy[final_tidy[\"metodo\"] == \"edificios\"].groupby([\"nombre_barrio\", \"peligrosidad\"])[\"fam_expuestas\"].sum().reset_index()\nedificios_data = edificios_data.rename(columns={\"fam_expuestas\": \"fam_expuestas_edificios\"})\n\n# Merge all methods together\nbarrio_summary = area_data.merge(ghsl_data, on=[\"nombre_barrio\", \"peligrosidad\"], how=\"outer\")\nbarrio_summary = barrio_summary.merge(edificios_data, on=[\"nombre_barrio\", \"peligrosidad\"], how=\"outer\")\n\n# Fill NaN values with 0\nbarrio_summary = barrio_summary.fillna(0)\n\n# Columns are already properly named from the aggregation\n\nfrom itables import show\n\n# Sort by nombre_barrio and peligrosidad in descending order\nbarrio_summary = barrio_summary.sort_values([\"nombre_barrio\", \"peligrosidad\"], ascending=True)\n\n# Display as interactive table with pagination\nshow(barrio_summary)\n\n\n\n\n    \n    \n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\n    Loading ITables v2.4.4 from the internet...\n    (need help?)\n    \n\n\n\n\nThis map is based on the “edificios” method, but could be replicated for the “area” or “GHSL” methods just as easily.\n\n\nMostrar código\n# Map 4: Barrios por población expuesta estimada\nfig4, ax4 = create_consistent_map(\"Barrios por población expuesta estimada\", common_bounds)\n\n# Get edificios method data from final_tidy, filtering out low risk\nedificios_data = final_tidy[\n    (final_tidy[\"metodo\"] == \"edificios\") & \n    (final_tidy[\"peligrosidad\"].isin([\"alta\", \"media\"]))\n].copy()\n\n# Convert to GeoDataFrame and reproject geometries to EPSG:3857 for proper plotting\nedificios_gdf = gpd.GeoDataFrame(edificios_data, geometry='geometry', crs='EPSG:5349')\nedificios_gdf = edificios_gdf.clip(la_plata)\nedificios_gdf_3857 = edificios_gdf.to_crs('EPSG:3857')\n\n# Import matplotlib colormap using the correct syntax\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\n\n# Get RdPu colormap using the correct syntax\nrdpu_cmap = cm.RdPu\n\n# Define colors for peligrosidad levels using RdPu colormap\npeligrosidad_colors = {\n    \"alta\": rdpu_cmap(0.8),    # Darker red-purple for high risk\n    \"media\": rdpu_cmap(0.4)    # Lighter red-purple for medium risk\n}\n\n# Define plotting order - medium risk first (underneath), high risk last (on top)\nplotting_order = [\"media\", \"alta\"]\n\n# Plot centroids for each barrio with jittering, in controlled order\nnp.random.seed(42)  # For reproducible results\n\nfor peligrosidad in plotting_order:\n    # Get data for this peligrosidad level\n    level_data = edificios_gdf_3857[edificios_gdf_3857[\"peligrosidad\"] == peligrosidad]\n    \n    for _, row in level_data.iterrows():\n        # Calculate centroid from the reprojected geometry\n        centroid = row[\"geometry\"].centroid\n        \n        # Add jitter\n        jitter_x = np.random.uniform(-200, 200)\n        jitter_y = np.random.uniform(-200, 200)\n        \n        x_pos = centroid.x + jitter_x\n        y_pos = centroid.y + jitter_y\n        \n        # Get color based on peligrosidad\n        color = peligrosidad_colors[row[\"peligrosidad\"]]\n        \n        # Size based on number of families exposed\n        size = max(10, row[\"fam_expuestas\"] * 2 + 10)\n        \n        # Plot the centroid as a circle with jitter - use color keyword instead of c\n        ax4.scatter(\n            x_pos, y_pos, \n            s=size, color=color,  # Changed from c=color to color=color\n            alpha=0.9,\n            edgecolors='white',\n            linewidth=1.0\n        )\n\n# Add legend for peligrosidad levels\nlegend_elements = [\n    plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=color, markersize=8, label=level.capitalize())\n    for level, color in peligrosidad_colors.items()\n]\nax4.legend(handles=legend_elements, title=\"Nivel de Peligrosidad\", loc='upper right')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n3.6.3 Exposición por cuenca\n\n\n3.6.4 Exposición por eje\n\n\n\n\nSchiavina, M., S. Freire, A. Carioli, and K. MacManus. 2023. “GHS-POP R2023A - GHS Population Grid Multitemporal (1975-2030).” European Commission, Joint Research Centre (JRC). https://doi.org/10.2905/2FF68A52-5B5B-4A22-8F40-C41DA8332CFE.\n\n\nSmith, A., P. D. Bates, O. Wing, et al. 2019. “New Estimates of Flood Exposure in Developing Countries Using High-Resolution Population Data.” Nature Communications 10: 1814. https://doi.org/10.1038/s41467-019-09282-y.\n\n\nTellman, B., J. A. Sullivan, C. Kuhn, et al. 2021. “Satellite Imaging Reveals Increased Proportion of Population Exposed to Floods.” Nature 596: 80–86. https://doi.org/10.1038/s41586-021-03695-w.\n\n\nVIDA. 2023. “Google-Microsoft-OSM Open Buildings - Combined by VIDA.” https://source.coop/repositories/vida/google-microsoft-osm-open-buildings/access.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>RENABAP</span>"
    ]
  },
  {
    "objectID": "renabap.html#fuentes-de-datos-y-metodología",
    "href": "renabap.html#fuentes-de-datos-y-metodología",
    "title": "3  RENABAP",
    "section": "3.2 Fuentes de datos y metodología",
    "text": "3.2 Fuentes de datos y metodología\n\n3.2.1 Datos\n\n3.2.1.1 RENABAP\n\n\n3.2.1.2 Censo Argentino\n\n\n3.2.1.3 GHSL\n\n\n3.2.1.4 Google-Microsoft Open Buildings\n\n\n\n3.2.2 Metodología\n\n3.2.2.1 Interpolación por area\n\n\n3.2.2.2 Mapeo dasymetrico",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>RENABAP</span>"
    ]
  },
  {
    "objectID": "renabap.html#procesamiento-y-resultados",
    "href": "renabap.html#procesamiento-y-resultados",
    "title": "3  RENABAP",
    "section": "3.3 Procesamiento y resultados",
    "text": "3.3 Procesamiento y resultados\n\n\nMostrar código\nimport pandas as pd\nimport geopandas as gpd\nimport requests\nfrom io import StringIO\n\nimport boto3\nimport duckdb\n\n\nimport matplotlib.pyplot as plt\n\nimport numpy as np\nimport s2sphere\nfrom botocore.config import Config\nfrom rasterstats import zonal_stats\n\n\nfrom shapely.geometry import box\n\n\nUSE_CRS = \"EPSG:5349\"\n\n\n\n### import data\n\nresponse = requests.get(\n    \"https://www.argentina.gob.ar/sites/default/files/renabap-2023-12-06.geojson\"\n)\nrenabap = gpd.read_file(StringIO(response.text))\nrenabap_pba = renabap[renabap[\"provincia\"] == \"Buenos Aires\"]\nrenabap_pba = renabap_pba.to_crs(USE_CRS)\n\npeligro_path = \"/home/nissim/Documents/dev/fulbright/ciut-riesgo/notebooks/data/la_plata_pelig_2023_datos_originales.geojson\"\npeligro = gpd.read_file(peligro_path)\npeligro = peligro.to_crs(USE_CRS)\n\npeligro_bounds = peligro.total_bounds\npeligro_bbox = box(*peligro_bounds)\n\nrenabap_pba_intersect = renabap_pba[\n    renabap_pba.geometry.intersects(peligro_bbox)\n].copy()\n\n\n\n3.3.1 Interpolación por area\n\n\nMostrar código\n# Ensure both GeoDataFrames have the same CRS\nif renabap_pba_intersect.crs != peligro.crs:\n    peligro = peligro.to_crs(renabap_pba_intersect.crs)\n\n# Get unique hazard levels\nhazard_levels = peligro[\"PELIGROSID\"].unique()\n\n# Initialize result columns\nrenabap_with_porciones = renabap_pba_intersect.copy()\nfor level in hazard_levels:\n    renabap_with_porciones[f\"porcion_{level}\"] = 0.0\n\n# Calculate total area of each barrio\nrenabap_with_porciones['total_area'] = renabap_with_porciones.geometry.area\n\n# For each barrio, calculate intersection with each hazard level\nfor idx, barrio in renabap_with_porciones.iterrows():\n    barrio_geom = barrio.geometry\n    barrio_total_area = barrio_geom.area\n    \n    if barrio_total_area == 0:\n        continue\n        \n    for level in hazard_levels:\n        hazard_subset = peligro[peligro[\"PELIGROSID\"] == level]\n        \n        if hazard_subset.empty:\n            continue\n        \n        # Calculate intersection area\n        intersection_area = 0\n        for _, hazard_row in hazard_subset.iterrows():\n            try:\n                intersection = barrio_geom.intersection(hazard_row.geometry)\n                if not intersection.is_empty:\n                    intersection_area += intersection.area\n            except Exception as e:\n                print(f\"Error calculating intersection for {barrio.get('nombre_barrio', idx)}: {e}\")\n                continue\n        \n        # Calculate proportion\n        proportion = intersection_area / barrio_total_area if barrio_total_area &gt; 0 else 0\n        renabap_with_porciones.at[idx, f\"porcion_{level}\"] = proportion\n\n# Create tidy format dataframe\nrenabap_tidy = []\n\nfor idx, row in renabap_with_porciones.iterrows():\n    for level in hazard_levels:\n        familias_expuestas = row[f\"porcion_{level}\"] * row[\"familias_aproximadas\"]\n        \n        renabap_tidy.append({\n            \"id_renabap\": row[\"id_renabap\"],\n            \"nombre_barrio\": row[\"nombre_barrio\"],\n            \"peligrosidad\": level,\n            \"fam_expuestas_areal\": familias_expuestas\n        })\n\nrenabap_tidy = pd.DataFrame(renabap_tidy)\n\nprint(renabap_tidy.head())\n\n\n   id_renabap nombre_barrio peligrosidad  fam_expuestas_areal\n0           2   Malvinas II         alta             0.000000\n1           2   Malvinas II         baja             1.001885\n2           2   Malvinas II        media             0.000000\n3           3   Ferroviario         alta             0.000000\n4           3   Ferroviario         baja            18.936166\n\n\n\n\n3.3.2 Cuenta de edificios\n\n\nMostrar código\ndef fetch_buildings(geodataframe, temp_file=\"buildings_filtered.parquet\"):\n    \"\"\"Fetch building data for a given GeoDataFrame region\"\"\"\n\n    # Get S2 cell and bounds\n    center = geodataframe.to_crs(\"epsg:3857\").union_all().centroid\n    center_wgs84 = (\n        gpd.GeoDataFrame(geometry=[center], crs=\"EPSG:3857\")\n        .to_crs(epsg=4326)\n        .geometry.iloc[0]\n    )\n    cell = s2sphere.CellId.from_lat_lng(\n        s2sphere.LatLng.from_degrees(center_wgs84.y, center_wgs84.x)\n    ).parent(10)\n    bounds = geodataframe.to_crs(\"epsg:4326\").total_bounds\n\n    # Find matching S2 partition\n    s3 = boto3.client(\n        \"s3\",\n        endpoint_url=\"https://data.source.coop\",\n        aws_access_key_id=\"\",\n        aws_secret_access_key=\"\",\n        config=Config(s3={\"addressing_style\": \"path\"}),\n    )\n\n    partitions = {\n        obj[\"Key\"].split(\"/\")[-1].replace(\".parquet\", \"\")\n        for obj in s3.list_objects_v2(\n            Bucket=\"vida\",\n            Prefix=\"google-microsoft-open-buildings/geoparquet/by_country_s2/country_iso=ARG/\",\n        ).get(\"Contents\", [])\n    }\n\n    parent_id = next(\n        str(cell.parent(level).id())\n        for level in range(10, 0, -1)\n        if str(cell.parent(level).id()) in partitions\n    )\n\n    # Setup DuckDB and query\n    con = duckdb.connect()\n    for cmd in [\n        \"INSTALL spatial\",\n        \"LOAD spatial\",\n        \"INSTALL httpfs\",\n        \"LOAD httpfs\",\n        \"SET s3_region='us-east-1'\",\n        \"SET s3_endpoint='data.source.coop'\",\n        \"SET s3_use_ssl=true\",\n        \"SET s3_url_style='path'\",\n    ]:\n        con.execute(cmd)\n\n    # Export and read back\n    query = f\"\"\"\n    COPY (SELECT * FROM 's3://vida/google-microsoft-open-buildings/geoparquet/by_country_s2/country_iso=ARG/{parent_id}.parquet'\n          WHERE bbox.xmax &gt;= {bounds[0]} AND bbox.xmin &lt;= {bounds[2]} AND\n                bbox.ymax &gt;= {bounds[1]} AND bbox.ymin &lt;= {bounds[3]}\n    ) TO '{temp_file}' (FORMAT PARQUET);\n    \"\"\"\n\n    con.execute(query)\n    df = pd.read_parquet(temp_file)\n    df[\"geometry\"] = gpd.GeoSeries.from_wkb(df[\"geometry\"])\n\n    return gpd.GeoDataFrame(df, geometry=\"geometry\", crs=\"EPSG:4326\")\n\n\n# Usage:\nbuildings = fetch_buildings(renabap_pba_intersect)\n\n\n\n\n\n\n\n3.3.3 Mapeo dasymetrico con datos GHSL\n\n\nMostrar código\nimport rioxarray\nfrom shapely.geometry import box\n\n# Load GHSL data with dask chunking for memory efficiency\nghsl = rioxarray.open_rasterio(\n    \"/home/nissim/Downloads/spatial/GHS_POP_E2025_GLOBE_R2023A_54009_100_V1_0_R14_C13/GHS_POP_E2025_GLOBE_R2023A_54009_100_V1_0_R14_C13.tif\",\n    chunks={\"x\": 1024, \"y\": 1024},  # Adjust chunk size based on your memory constraints\n)\n\n# Reproject to your target CRS with streaming\nghsl = ghsl.rio.reproject(dst_crs=USE_CRS)\n\n# Clip to renabap_pba_intersect bounding box using streaming\nbounding_box = box(\n    *renabap_pba_intersect.total_bounds\n)  # Create a box from the bounding box coordinates\n\nghsl_clipped = ghsl.rio.clip(\n    [bounding_box],  # Use the bounding box as a geometry (wrapped in a list)\n    from_disk=True,  # Process from disk to avoid loading entire dataset into memory\n)\n\n\nimport rasterstats\n\n# Step 1: Calculate the total GHSL population per barrio popular using zonal statistics\nprint(\"Calculating GHSL population totals per barrio popular...\")\n\n# Convert to the format expected by rasterstats\ngeometries = [geom for geom in renabap_pba_intersect.geometry]\n\n# Use rasterstats for vectorized zonal statistics\nstats = rasterstats.zonal_stats(\n    geometries,\n    ghsl_clipped.values[0],  # rasterstats expects 2D array\n    affine=ghsl_clipped.rio.transform(),\n    stats=[\"sum\"],\n    nodata=ghsl_clipped.rio.nodata,\n)\n\n# Extract the sum values\nghsl_totals = [stat[\"sum\"] if stat[\"sum\"] is not None else 0 for stat in stats]\n\n# Add the GHSL population estimates as a new column\nrenabap_pba_intersect[\"ghsl_pop_est\"] = ghsl_totals\n\n# Verify the results\nprint(f\"Added GHSL population estimates to {len(ghsl_totals)} barrios\")\nprint(f\"Total estimated population: {sum(ghsl_totals):,.0f}\")\nprint(f\"Range: {min(ghsl_totals):.0f} - {max(ghsl_totals):.0f}\")\n\n# Show a few examples\nprint(\"\\nFirst 5 barrios with GHSL estimates:\")\nprint(renabap_pba_intersect[[\"geometry\", \"ghsl_pop_est\"]].head())\n\nfrom rasterio.features import rasterize\nimport numpy as np\n\n# Get the reference raster properties from GHSL data\nreference_raster = ghsl_clipped\nreference_transform = reference_raster.rio.transform()\nreference_crs = reference_raster.rio.crs\nreference_shape = reference_raster.shape[1:]  # Get 2D shape (height, width)\n\nprint(f\"Reference raster shape: {reference_shape}\")\nprint(f\"Reference CRS: {reference_crs}\")\n\n# Prepare geometries and values for rasterization\ngeometries_ghsl = [\n    (geom, value)\n    for geom, value in zip(\n        renabap_pba_intersect.geometry, renabap_pba_intersect[\"ghsl_pop_est\"]\n    )\n]\ngeometries_familias = [\n    (geom, value)\n    for geom, value in zip(\n        renabap_pba_intersect.geometry, renabap_pba_intersect[\"familias_aproximadas\"]\n    )\n]\n\n# Create GHSL population raster\nghsl_pop_raster = rasterize(\n    geometries_ghsl,\n    out_shape=reference_shape,\n    transform=reference_transform,\n    fill=0,\n    dtype=np.float32,\n    all_touched=False,\n)\n\n# Create familias aproximadas raster\nfamilias_raster = rasterize(\n    geometries_familias,\n    out_shape=reference_shape,\n    transform=reference_transform,\n    fill=0,\n    dtype=np.float32,\n    all_touched=False,\n)\n\n# Verify the rasters\nprint(\n    f\"GHSL population range: {np.nanmin(ghsl_pop_raster):.2f} - {np.nanmax(ghsl_pop_raster):.2f}\"\n)\nprint(\n    f\"Familias range: {np.nanmin(familias_raster):.2f} - {np.nanmax(familias_raster):.2f}\"\n)\n\n# Step 1: Divide original GHSL by the barrio-level GHSL to get fractional population\n# Use masking to avoid division on invalid cells\nmask = (ghsl_clipped.values[0] != -200) & (ghsl_pop_raster &gt; 0.1)\nghsl_fractional = np.full_like(ghsl_clipped.values[0], -200, dtype=np.float64)\nghsl_fractional[mask] = ghsl_clipped.values[0][mask] / ghsl_pop_raster[mask]\n\n# Step 2: Multiply fractional population by familias aproximadas to get downscaled data\nmask2 = (ghsl_fractional != -200) & (familias_raster &gt; 0)\nfamilias_downscaled = np.full_like(ghsl_clipped.values[0], -200, dtype=np.float64)\nfamilias_downscaled[mask2] = ghsl_fractional[mask2] * familias_raster[mask2]\n\n# Verify the results - exclude -200 from range calculations\nghsl_valid = ghsl_clipped.values[0] != -200\nfractional_valid = ghsl_fractional != -200\ndownscaled_valid = familias_downscaled != -200\n\nprint(\n    f\"Original GHSL range: {np.min(ghsl_clipped.values[0][ghsl_valid]):.2f} - {np.max(ghsl_clipped.values[0][ghsl_valid]):.2f}\"\n)\nprint(\n    f\"Fractional population range: {np.min(ghsl_fractional[fractional_valid]):.4f} - {np.max(ghsl_fractional[fractional_valid]):.4f}\"\n)\nprint(\n    f\"Downscaled familias range: {np.min(familias_downscaled[downscaled_valid]):.2f} - {np.max(familias_downscaled[downscaled_valid]):.2f}\"\n)\n\n# Check that the sum of downscaled familias equals the original familias aproximadas\ntotal_original_familias = renabap_pba_intersect[\"familias_aproximadas\"].sum()\ntotal_downscaled_familias = np.sum(familias_downscaled[downscaled_valid])\nprint(f\"\\nTotal original familias: {total_original_familias:,.0f}\")\nprint(f\"Total downscaled familias: {total_downscaled_familias:,.0f}\")\nprint(f\"Difference: {abs(total_original_familias - total_downscaled_familias):,.2f}\")\n\n# Intersect settlements with hazard zones\nsettlement_hazard = gpd.overlay(renabap_pba_intersect, peligro, how=\"intersection\")\n\n# Create GHSL tidy dataframe with matching structure\nghsl_tidy = []\n\nfor idx, row in settlement_hazard.iterrows():\n    stats = zonal_stats(\n        [row.geometry],\n        familias_downscaled,  # your numpy array\n        affine=reference_transform,  # get transform from your xarray\n        stats=[\"sum\"],\n        nodata=-200,  # use your actual nodata value\n    )[0]\n\n    ghsl_tidy.append(\n        {\n            \"id_renabap\": row[\"id_renabap\"],\n            \"peligrosidad\": row[\"PELIGROSID\"],\n            \"fam_expuestas_ghsl\": stats[\"sum\"] if stats[\"sum\"] is not None else 0,\n        }\n    )\n\nghsl_tidy = pd.DataFrame(ghsl_tidy)\n\nprint(ghsl_tidy.head())\n\n\nCalculating GHSL population totals per barrio popular...\nAdded GHSL population estimates to 323 barrios\nTotal estimated population: 257,167\nRange: 0 - 9332\n\nFirst 5 barrios with GHSL estimates:\n                                            geometry  ghsl_pop_est\n1  MULTIPOLYGON (((7133413.584 6125858.719, 71336...    729.961555\n2  MULTIPOLYGON (((7127613.216 6115863.573, 71276...    240.369900\n3  MULTIPOLYGON (((7136174.621 6130304.579, 71361...    265.001049\n4  MULTIPOLYGON (((7128546.05 6130775.49, 7128519...     10.639407\n5  MULTIPOLYGON (((7129976.453 6132330.669, 71299...      0.000000\nReference raster shape: (490, 711)\nReference CRS: EPSG:5349\nGHSL population range: 0.00 - 9332.04\nFamilias range: 0.00 - 2640.00\nOriginal GHSL range: 0.00 - 304.95\nFractional population range: 0.0000 - 1.0000\nDownscaled familias range: 0.00 - 383.00\n\nTotal original familias: 88,856\nTotal downscaled familias: 88,680\nDifference: 176.00\n   id_renabap peligrosidad  fam_expuestas_ghsl\n0           2         baja            0.000000\n1           3         baja           14.286419\n2           3        media           32.931858\n3           4         alta            0.000000\n4           4        media          134.000006\n\n\n\n\n3.3.4 Mapeo dasymetrico con datos de edificios\n\n\nMostrar código\n# Reproject buildings to match the analysis CRS\nbuildings_proj = buildings.to_crs(USE_CRS)\n\n# Step 1: Calculate buildings per settlement-hazard intersection\nbuildings_hazard = gpd.overlay(buildings_proj, settlement_hazard, how=\"intersection\")\n\n# Count buildings per settlement-hazard combination\nbuildings_per_hazard = (\n    buildings_hazard.groupby([\"id_renabap\", \"PELIGROSID\"])\n    .size()\n    .reset_index(name=\"buildings_count\")\n)\n\n# Step 2: Calculate total buildings per settlement (barrio popular)\nbuildings_settlement = gpd.overlay(\n    buildings_proj, renabap_pba_intersect, how=\"intersection\"\n)\ntotal_buildings_per_settlement = (\n    buildings_settlement.groupby(\"id_renabap\")\n    .size()\n    .reset_index(name=\"total_buildings\")\n)\n\n# Step 3: Merge and calculate ratios\nhazard_ratios = buildings_per_hazard.merge(\n    total_buildings_per_settlement, on=\"id_renabap\", how=\"left\"\n)\nhazard_ratios[\"building_ratio\"] = (\n    hazard_ratios[\"buildings_count\"] / hazard_ratios[\"total_buildings\"]\n)\n\n# Step 4: Get total population per settlement and apply ratios\nsettlement_population = renabap_pba_intersect[\n    [\"id_renabap\", \"familias_aproximadas\"]\n].copy()\n\n# Merge with ratios and calculate population estimates\npopulation_estimates = hazard_ratios.merge(\n    settlement_population, on=\"id_renabap\", how=\"left\"\n)\npopulation_estimates[\"estimated_population_hazard\"] = (\n    population_estimates[\"building_ratio\"]\n    * population_estimates[\"familias_aproximadas\"]\n)\n\n# Step 5: Create final results with totals\nfinal_results = population_estimates[\n    [\"id_renabap\", \"PELIGROSID\", \"estimated_population_hazard\"]\n].copy()\n\n# Add total population rows (no hazard breakdown)\ntotal_pop_rows = settlement_population.copy()\ntotal_pop_rows[\"PELIGROSID\"] = \"total\"\ntotal_pop_rows[\"estimated_population_hazard\"] = total_pop_rows[\"familias_aproximadas\"]\n\n# Combine\nfinal_results = pd.concat(\n    [\n        final_results,\n        total_pop_rows[[\"id_renabap\", \"PELIGROSID\", \"estimated_population_hazard\"]],\n    ],\n    ignore_index=True,\n)\n\n# Create buildings tidy dataframe with matching structure\nbuildings_tidy = final_results[\n    [\"id_renabap\", \"PELIGROSID\", \"estimated_population_hazard\"]\n].copy()\n\n# Rename columns to match the structure\nbuildings_tidy = buildings_tidy.rename(\n    columns={\n        \"PELIGROSID\": \"peligrosidad\",\n        \"estimated_population_hazard\": \"fam_expuestas_edificios\",\n    }\n)\n\n# Filter out the 'total' rows since we only want hazard-specific data\nbuildings_tidy = buildings_tidy[buildings_tidy[\"peligrosidad\"] != \"total\"].copy()\n\nprint(buildings_tidy.head())\n\n\n   id_renabap peligrosidad  fam_expuestas_edificios\n0           2         baja                 3.538827\n1           3         baja                33.654237\n2           3        media                22.766102\n3           4         alta                36.258824\n4           4        media               122.964706",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>RENABAP</span>"
    ]
  },
  {
    "objectID": "renabap.html#conclusions",
    "href": "renabap.html#conclusions",
    "title": "3  RENABAP",
    "section": "3.4 Conclusions",
    "text": "3.4 Conclusions\n\n3.4.1 Comparación de resuldatos\n\n\nMostrar código\n# Join all three dataframes by id_renabap and peligrosidad\nfinal_df = renabap_tidy.merge(\n    ghsl_tidy, on=[\"id_renabap\", \"peligrosidad\"], how=\"outer\"\n)\nfinal_df = final_df.merge(\n    buildings_tidy, on=[\"id_renabap\", \"peligrosidad\"], how=\"outer\"\n)\n\n# Impute 0s for NA values in fam_expuestas columns\nfam_expuestas_columns = [col for col in final_df.columns if 'fam_expuestas' in col]\nfinal_df[fam_expuestas_columns] = final_df[fam_expuestas_columns].fillna(0)\n\n# Create long format dataframe with aggregation\nfinal_tidy = []\n\n# Add renabap data\nfor _, row in renabap_tidy.iterrows():\n    final_tidy.append(\n        {\n            \"id_renabap\": row[\"id_renabap\"],\n            \"peligrosidad\": row[\"peligrosidad\"],\n            \"metodo\": \"area\",\n            \"fam_expuestas\": row[\"fam_expuestas_areal\"],\n        }\n    )\n\n# Add ghsl data\nfor _, row in ghsl_tidy.iterrows():\n    final_tidy.append(\n        {\n            \"id_renabap\": row[\"id_renabap\"],\n            \"peligrosidad\": row[\"peligrosidad\"],\n            \"metodo\": \"ghsl\",\n            \"fam_expuestas\": row[\"fam_expuestas_ghsl\"],\n        }\n    )\n\n# Add buildings data\nfor _, row in buildings_tidy.iterrows():\n    final_tidy.append(\n        {\n            \"id_renabap\": row[\"id_renabap\"],\n            \"peligrosidad\": row[\"peligrosidad\"],\n            \"metodo\": \"edificios\",\n            \"fam_expuestas\": row[\"fam_expuestas_edificios\"],\n        }\n    )\n\nfinal_tidy = pd.DataFrame(final_tidy)\n\n# Aggregate to get one observation per barrio per hazard level per method\nfinal_tidy = (\n    final_tidy.groupby([\"id_renabap\", \"peligrosidad\", \"metodo\"])[\"fam_expuestas\"]\n    .sum()\n    .reset_index()\n)\n\n# Create complete combination of all barrios, hazard levels, and methods\nall_barrios = final_tidy[\"id_renabap\"].unique()\nall_hazard_levels = [\"alta\", \"baja\", \"media\"]\nall_methods = [\"area\", \"ghsl\", \"edificios\"]\n\ncomplete_combinations = pd.DataFrame([\n    {\"id_renabap\": barrio, \"peligrosidad\": hazard, \"metodo\": method}\n    for barrio in all_barrios\n    for hazard in all_hazard_levels\n    for method in all_methods\n])\n\n# Merge with actual data and fill missing values with 0\nfinal_tidy = complete_combinations.merge(\n    final_tidy, on=[\"id_renabap\", \"peligrosidad\", \"metodo\"], how=\"left\"\n)\nfinal_tidy[\"fam_expuestas\"] = final_tidy[\"fam_expuestas\"].fillna(0)\n\nprint(final_tidy.head(10))\nprint(f\"Shape: {final_tidy.shape}\")\n\n# Calculate total exposure per hazard level per method\nsummary = (\n    final_tidy.groupby([\"peligrosidad\", \"metodo\"])[\"fam_expuestas\"]\n    .sum()\n    .reset_index()\n    .pivot(index=\"peligrosidad\", columns=\"metodo\", values=\"fam_expuestas\")\n)\n\nprint(\"Total Familias Expuestas por Peligrosidad y Método:\")\nprint(\"=\" * 50)\nprint(summary.round(2))\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Filter for high exposure (alta peligrosidad)\nalta_data = final_tidy[final_tidy[\"peligrosidad\"] == \"alta\"].copy()\n\n# Calculate total exposure per barrio across all methods\ntotal_exposure = (\n    alta_data.groupby(\"id_renabap\")[\"fam_expuestas\"]\n    .sum()\n    .sort_values(ascending=False)\n)\ntop_25_barrios = total_exposure.head(25).index\n\n# Filter data for top 25 barrios\ntop_25_data = alta_data[\n    alta_data[\"id_renabap\"].isin(top_25_barrios)\n].copy()\n\n# Create range plot showing min, max, and individual points\nplt.figure(figsize=(15, 10))\n\n# Define colors for methods\nmethod_colors = {\"area\": \"blue\", \"ghsl\": \"red\", \"edificios\": \"green\"}\n\nfor i, barrio in enumerate(top_25_barrios):\n    barrio_data = top_25_data[top_25_data[\"id_renabap\"] == barrio]\n    if len(barrio_data) &gt; 0:\n        values = barrio_data[\"fam_expuestas\"].values\n        min_val = values.min()\n        max_val = values.max()\n\n        # Plot range line\n        plt.plot([min_val, max_val], [i, i], \"k-\", alpha=0.5, linewidth=2)\n\n        # Plot individual points colored by method\n        for _, row in barrio_data.iterrows():\n            color = method_colors[row[\"metodo\"]]\n            plt.plot(row[\"fam_expuestas\"], i, \"o\", color=color, markersize=6, alpha=0.8)\n\nplt.yticks(range(len(top_25_barrios)), top_25_barrios)\nplt.xlabel(\"Familias Expuestas\")\nplt.ylabel(\"Barrio ID\")\nplt.title(\"Range of High Exposure Estimates for Top 25 Barrios\", fontsize=14)\nplt.grid(True, alpha=0.3)\n\n# Add legend\nlegend_elements = [\n    plt.Line2D(\n        [0],\n        [0],\n        marker=\"o\",\n        color=\"w\",\n        markerfacecolor=color,\n        markersize=8,\n        label=method,\n    )\n    for method, color in method_colors.items()\n]\nplt.legend(handles=legend_elements, title=\"Método\")\n\nplt.tight_layout()\nplt.show()\n\n\n   id_renabap peligrosidad     metodo  fam_expuestas\n0           2         alta       area       0.000000\n1           2         alta       ghsl       0.000000\n2           2         alta  edificios       0.000000\n3           2         baja       area       1.001885\n4           2         baja       ghsl       0.000000\n5           2         baja  edificios       3.538827\n6           2        media       area       0.000000\n7           2        media       ghsl       0.000000\n8           2        media  edificios       0.000000\n9           3         alta       area       0.000000\nShape: (2907, 4)\nTotal Familias Expuestas por Peligrosidad y Método:\n==================================================\nmetodo           area  edificios     ghsl\npeligrosidad                             \nalta          3552.36    3646.34  2831.97\nbaja          7581.05    9911.60  7726.58\nmedia         8555.48    9678.24  8400.36",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>RENABAP</span>"
    ]
  },
  {
    "objectID": "renabap.html#referencias",
    "href": "renabap.html#referencias",
    "title": "3  RENABAP",
    "section": "3.5 Referencias",
    "text": "3.5 Referencias\n\n\n\n\nSchiavina, M., S. Freire, A. Carioli, and K. MacManus. 2023. “GHS-POP R2023A - GHS Population Grid Multitemporal (1975-2030).” European Commission, Joint Research Centre (JRC). https://doi.org/10.2905/2FF68A52-5B5B-4A22-8F40-C41DA8332CFE.\n\n\nSmith, A., P. D. Bates, O. Wing, et al. 2019. “New Estimates of Flood Exposure in Developing Countries Using High-Resolution Population Data.” Nature Communications 10: 1814. https://doi.org/10.1038/s41467-019-09282-y.\n\n\nTellman, B., J. A. Sullivan, C. Kuhn, et al. 2021. “Satellite Imaging Reveals Increased Proportion of Population Exposed to Floods.” Nature 596: 80–86. https://doi.org/10.1038/s41586-021-03695-w.\n\n\nVIDA. 2023. “Google-Microsoft-OSM Open Buildings - Combined by VIDA.” https://source.coop/repositories/vida/google-microsoft-osm-open-buildings/access.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>RENABAP</span>"
    ]
  },
  {
    "objectID": "renabap.html#fuentes-de-datos",
    "href": "renabap.html#fuentes-de-datos",
    "title": "3  RENABAP",
    "section": "3.4 Fuentes de datos",
    "text": "3.4 Fuentes de datos\n\n3.4.1 RENABAP\nEl Registro Nacional de Barrios Populares (RENABAP) es producido por la Subsecretaría de Integración Socio Urbana y proporciona información sobre asentamientos informales en Argentina, incluyendo estimaciones de población y delimitaciones geográficas de estos barrios. Más información sobre el RENABAP está disponible en el Observatorio de Barrios Populares. Los datos fueron obtenidos a través del Mapa de Barrios Populares y están disponibles para descarga como GeoJSON.\n\n\nMostrar código\nfig1, ax1 = create_consistent_map(\"Asentamientos RENABAP en La Plata\", common_bounds)\n\n# Add RENABAP settlements as black outline with no fill\n# Reproject to Web Mercator to match the map's coordinate system\nrenabap_pba_intersect_3857 = renabap_pba_intersect.to_crs(\"EPSG:3857\")\n\nrenabap_pba_intersect_3857.plot(\n    ax=ax1,\n    facecolor='none',\n    edgecolor='black',\n    linewidth=0.5,\n    legend=False,\n    zorder=10\n)\n\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n3.4.2 GHSL\nLa Capa Global de Asentamientos Humanos (Global Human Settlement Layer) (Schiavina et al. 2023) es un conjunto de datos de resolución de 100 metros que proporciona estimaciones de población multitemporales (1975-2030) derivadas de datos censales y administrativos, informadas por la distribución y clasificación de áreas construidas. El GHSL ya tiene un uso científico establecido para mapear la exposición poblacional a peligros de inundación (Tellman et al. 2021). Sin embargo, esta fuente presenta limitaciones importantes: estudios sobre modelado de riesgo de inundación con conjuntos de datos globales han demostrado que evaluar la exposición a esta escala de resolución puede llevar a sobreestimaciones de la exposición poblacional en zonas de peligro de inundación en comparación con datos de mayor resolución (Smith et al. 2019).\n\n\nMostrar código\nimport rioxarray\nfrom shapely.geometry import box\n\n# Load GHSL data with dask chunking for memory efficiency\nghsl = rioxarray.open_rasterio(\n    \"/home/nissim/Downloads/spatial/GHS_POP_E2025_GLOBE_R2023A_54009_100_V1_0_R14_C13/GHS_POP_E2025_GLOBE_R2023A_54009_100_V1_0_R14_C13.tif\",\n    chunks={\"x\": 1024, \"y\": 1024},  # Adjust chunk size based on your memory constraints\n)\n\n# Reproject to your target CRS with streaming\nghsl = ghsl.rio.reproject(dst_crs=USE_CRS)\n\n# Clip GHSL data to ONLY the Partido de La Plata boundaries\n# This will remove any GHSL data outside the partido\nghsl_clipped = ghsl.rio.clip(\n    [la_plata.geometry.iloc[0]],  # Use the actual La Plata partido geometry\n    from_disk=True,  # Process from disk to avoid loading entire dataset into memory\n)\n\n\n\n\nMostrar código\n# Import config for color palette\nfrom config import PINK_PALETTE\nfrom jenkspy import jenks_breaks\n\n# Map 2: GHSL population data\nfig2, ax2 = create_consistent_map(\"Datos de población GHSL\", common_bounds)\n\n# Create masked array to hide zero values (do this before jenks classification)\nimport numpy.ma as ma\nghsl_masked = ma.masked_where(ghsl_clipped.values[0] == 0, ghsl_clipped.values[0])\n\n# Use continuous log scale for GHSL data (excluding nodata values and zeros)\nghsl_valid = (ghsl_clipped.values[0] != -200) & (ghsl_clipped.values[0] != 0)\nghsl_valid_data = ghsl_clipped.values[0][ghsl_valid]\n\n# Use plasma colormap from matplotlib\nimport matplotlib.pyplot as plt\nimport matplotlib.colors as mcolors\nplasma_cmap = plt.cm.plasma\n\n# Reproject GHSL data to Web Mercator to match the map's coordinate system\nghsl_clipped_3857 = ghsl_clipped.rio.reproject(\"EPSG:3857\")\n\n# Mask out zeros AND nodata values\nghsl_masked_3857 = ma.masked_where(\n    (ghsl_clipped_3857.values[0] == 0) | (ghsl_clipped_3857.values[0] == -200),\n    ghsl_clipped_3857.values[0]\n)\n\n# Plot GHSL raster with continuous log scale and zero masking\n# Use power normalization to emphasize distinctions without extreme compression\nim = ax2.imshow(\n    ghsl_masked_3857,\n    extent=[ghsl_clipped_3857.x.min(), ghsl_clipped_3857.x.max(), \n            ghsl_clipped_3857.y.min(), ghsl_clipped_3857.y.max()],\n    cmap=plasma_cmap,\n    alpha=0.75\n)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n3.4.3 Google-Microsoft-OSM Open Buildings\nLos datos de Google-Microsoft-OSM Open Buildings - combined by VIDA (VIDA 2023) representan una forma más precisa de evaluar dónde se ubican los asentamientos humanos. Este conjunto de datos combina Google’s V3 Open Buildings, Microsoft’s GlobalMLFootprints, y OpenStreetMap building footprints, conteniendo más de 2.7 mil millones de huellas de edificios. Estos datos han sido exitosamente aplicados a evaluaciones de riesgo de inundación por empresas globales de riesgo financiero como ICE, demostrando su utilidad para mapear la exposición climática a nivel de huella de edificio individual. Sin embargo, en ausencia de información sobre si los edificios son residenciales o tienen otros usos, y sin datos sobre el número total de unidades en el edificio y habitantes por edificio, solo podemos obtener estimaciones proporcionales aproximadas de dónde se ubican las personas, sin tener una comprensión precisa de quién vive realmente allí y cuántas personas.\n\n\nMostrar código\ndef fetch_buildings(geodataframe, temp_file=\"buildings_filtered.parquet\"):\n    \"\"\"Fetch building data for a given GeoDataFrame region\"\"\"\n\n    # Get S2 cell and bounds\n    center = geodataframe.to_crs(\"epsg:3857\").union_all().centroid\n    center_wgs84 = (\n        gpd.GeoDataFrame(geometry=[center], crs=\"EPSG:3857\")\n        .to_crs(epsg=4326)\n        .geometry.iloc[0]\n    )\n    cell = s2sphere.CellId.from_lat_lng(\n        s2sphere.LatLng.from_degrees(center_wgs84.y, center_wgs84.x)\n    ).parent(10)\n    bounds = geodataframe.to_crs(\"epsg:4326\").total_bounds\n\n    # Find matching S2 partition\n    s3 = boto3.client(\n        \"s3\",\n        endpoint_url=\"https://data.source.coop\",\n        aws_access_key_id=\"\",\n        aws_secret_access_key=\"\",\n        config=Config(s3={\"addressing_style\": \"path\"}),\n    )\n\n    partitions = {\n        obj[\"Key\"].split(\"/\")[-1].replace(\".parquet\", \"\")\n        for obj in s3.list_objects_v2(\n            Bucket=\"vida\",\n            Prefix=\"google-microsoft-osm-open-buildings/geoparquet/by_country_s2/country_iso=ARG/\",\n        ).get(\"Contents\", [])\n    }\n\n    parent_id = next(\n        str(cell.parent(level).id())\n        for level in range(10, 0, -1)\n        if str(cell.parent(level).id()) in partitions\n    )\n\n    # Setup DuckDB and query\n    con = duckdb.connect()\n    for cmd in [\n        \"INSTALL spatial\",\n        \"LOAD spatial\",\n        \"INSTALL httpfs\",\n        \"LOAD httpfs\",\n        \"SET s3_region='us-east-1'\",\n        \"SET s3_endpoint='data.source.coop'\",\n        \"SET s3_use_ssl=true\",\n        \"SET s3_url_style='path'\",\n    ]:\n        con.execute(cmd)\n\n    # Export and read back\n    query = f\"\"\"\n    COPY (SELECT * FROM 's3://vida/google-microsoft-osm-open-buildings/geoparquet/by_country_s2/country_iso=ARG/{parent_id}.parquet'\n          WHERE bbox.xmax &gt;= {bounds[0]} AND bbox.xmin &lt;= {bounds[2]} AND\n                bbox.ymax &gt;= {bounds[1]} AND bbox.ymin &lt;= {bounds[3]}\n    ) TO '{temp_file}' (FORMAT PARQUET);\n    \"\"\"\n\n    con.execute(query)\n    df = pd.read_parquet(temp_file)\n    df[\"geometry\"] = gpd.GeoSeries.from_wkb(df[\"geometry\"])\n\n    return gpd.GeoDataFrame(df, geometry=\"geometry\", crs=\"EPSG:4326\")\n\n\n# Usage:\nbuildings = fetch_buildings(renabap_pba_intersect)\n\n# Reproject buildings to match the analysis CRS\nbuildings_proj = buildings.to_crs(USE_CRS)\n\n# clip the buildings to the partido de la plata\nbuildings_proj = buildings_proj.clip(la_plata)\n\n\n\n\n\n\n\nMostrar código\n# Map 3: Building footprints\nfig3, ax3 = create_consistent_map(\"Huellas de edificios\", common_bounds)\n\n# Add building footprints with grey fill and no outline\nbuildings_3857 = buildings_proj.to_crs(\"EPSG:3857\")\n\nbuildings_3857.plot(\n    ax=ax3,\n    facecolor='grey',\n    edgecolor='none',\n    alpha=0.7\n)\n\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>RENABAP</span>"
    ]
  },
  {
    "objectID": "renabap.html#metodología-y-procesamiento",
    "href": "renabap.html#metodología-y-procesamiento",
    "title": "3  RENABAP",
    "section": "3.5 Metodología y procesamiento",
    "text": "3.5 Metodología y procesamiento\n\n3.5.1 Interpolación por area\nLa interpolación areal es un método simple en el que las variables de los datos fuente se ponderan según la superposición entre polígonos fuente y objetivo, luego se reagregan para ajustarse a las geometrías de los polígonos objetivo. En nuestro análisis, esto significa distribuir proporcionalmente la población de cada barrio popular según el área de intersección con diferentes niveles de peligro de inundación. El analasis original de la exposición poblacional a peligros de inundación en la región del Partido de La Plata se realizó utilizando este método.\n\n\nMostrar código\n# Ensure both GeoDataFrames have the same CRS\nif renabap_pba_intersect.crs != peligro.crs:\n    peligro = peligro.to_crs(renabap_pba_intersect.crs)\n\n# Get unique hazard levels\nhazard_levels = peligro[\"PELIGROSID\"].unique()\n\n# Initialize result columns\nrenabap_with_porciones = renabap_pba_intersect.copy()\nfor level in hazard_levels:\n    renabap_with_porciones[f\"porcion_{level}\"] = 0.0\n\n# Calculate total area of each barrio\nrenabap_with_porciones['total_area'] = renabap_with_porciones.geometry.area\n\n# For each barrio, calculate intersection with each hazard level\nfor idx, barrio in renabap_with_porciones.iterrows():\n    barrio_geom = barrio.geometry\n    barrio_total_area = barrio_geom.area\n    \n    if barrio_total_area == 0:\n        continue\n        \n    for level in hazard_levels:\n        hazard_subset = peligro[peligro[\"PELIGROSID\"] == level]\n        \n        if hazard_subset.empty:\n            continue\n        \n        # Calculate intersection area\n        intersection_area = 0\n        for _, hazard_row in hazard_subset.iterrows():\n            try:\n                intersection = barrio_geom.intersection(hazard_row.geometry)\n                if not intersection.is_empty:\n                    intersection_area += intersection.area\n            except Exception as e:\n                print(f\"Error calculating intersection for {barrio.get('nombre_barrio', idx)}: {e}\")\n                continue\n        \n        # Calculate proportion\n        proportion = intersection_area / barrio_total_area if barrio_total_area &gt; 0 else 0\n        renabap_with_porciones.at[idx, f\"porcion_{level}\"] = proportion\n\n# Create tidy format dataframe\nrenabap_tidy = []\n\nfor idx, row in renabap_with_porciones.iterrows():\n    for level in hazard_levels:\n        familias_expuestas = row[f\"porcion_{level}\"] * row[\"familias_aproximadas\"]\n        \n        renabap_tidy.append({\n            \"id_renabap\": row[\"id_renabap\"],\n            \"nombre_barrio\": row[\"nombre_barrio\"],\n            \"peligrosidad\": level,\n            \"fam_expuestas_areal\": familias_expuestas\n        })\n\nrenabap_tidy = pd.DataFrame(renabap_tidy)\n\n\n\n\n3.5.2 Cuenta de edificios\n\n\n3.5.3 Mapeo dasymetrico\nEl mapeo dasimétrico reorganiza datos cartográficos de una unidad de recolección en áreas más precisas, modificando los límites originales usando datos de apoyo relacionados. Por ejemplo, un atributo de población organizado por tracto censal se vuelve más significativo cuando se eliminan áreas donde es razonable inferir que la gente no vive (cuerpos de agua, terrenos vacíos). En nuestro caso, utilizamos datos GHSL y huellas de edificios como información auxiliar para mejorar la precisión de las estimaciones de distribución poblacional.\n\n3.5.3.1 Mapeo dasymetrico con datos de edificios\n\n\nMostrar código\n# Step 1: Calculate buildings per settlement-hazard intersection\nbuildings_hazard = gpd.overlay(buildings_proj, settlement_hazard, how=\"intersection\")\n\n# Count buildings per settlement-hazard combination\nbuildings_per_hazard = (\n    buildings_hazard.groupby([\"id_renabap\", \"PELIGROSID\"])\n    .size()\n    .reset_index(name=\"buildings_count\")\n)\n\n# Step 2: Calculate total buildings per settlement (barrio popular)\nbuildings_settlement = gpd.overlay(\n    buildings_proj, renabap_pba_intersect, how=\"intersection\"\n)\ntotal_buildings_per_settlement = (\n    buildings_settlement.groupby(\"id_renabap\")\n    .size()\n    .reset_index(name=\"total_buildings\")\n)\n\n# Step 3: Merge and calculate ratios\nhazard_ratios = buildings_per_hazard.merge(\n    total_buildings_per_settlement, on=\"id_renabap\", how=\"left\"\n)\nhazard_ratios[\"building_ratio\"] = (\n    hazard_ratios[\"buildings_count\"] / hazard_ratios[\"total_buildings\"]\n)\n\n# Step 4: Get total population per settlement and apply ratios\nsettlement_population = renabap_pba_intersect[\n    [\"id_renabap\", \"familias_aproximadas\"]\n].copy()\n\n# Merge with ratios and calculate population estimates\npopulation_estimates = hazard_ratios.merge(\n    settlement_population, on=\"id_renabap\", how=\"left\"\n)\npopulation_estimates[\"estimated_population_hazard\"] = (\n    population_estimates[\"building_ratio\"]\n    * population_estimates[\"familias_aproximadas\"]\n)\n\n# Step 5: Create final results with totals\nfinal_results = population_estimates[\n    [\"id_renabap\", \"PELIGROSID\", \"estimated_population_hazard\"]\n].copy()\n\n# Add total population rows (no hazard breakdown)\ntotal_pop_rows = settlement_population.copy()\ntotal_pop_rows[\"PELIGROSID\"] = \"total\"\ntotal_pop_rows[\"estimated_population_hazard\"] = total_pop_rows[\"familias_aproximadas\"]\n\n# Combine\nfinal_results = pd.concat(\n    [\n        final_results,\n        total_pop_rows[[\"id_renabap\", \"PELIGROSID\", \"estimated_population_hazard\"]],\n    ],\n    ignore_index=True,\n)\n\n# Create buildings tidy dataframe with matching structure\nbuildings_tidy = final_results[\n    [\"id_renabap\", \"PELIGROSID\", \"estimated_population_hazard\"]\n].copy()\n\n# Rename columns to match the structure\nbuildings_tidy = buildings_tidy.rename(\n    columns={\n        \"PELIGROSID\": \"peligrosidad\",\n        \"estimated_population_hazard\": \"fam_expuestas_edificios\",\n    }\n)\n\n# Filter out the 'total' rows since we only want hazard-specific data\nbuildings_tidy = buildings_tidy[buildings_tidy[\"peligrosidad\"] != \"total\"].copy()\n\n\n\n\n3.5.3.2 Mapeo dasymetrico con datos GHSL\n\n\nMostrar código\nimport rasterstats\n\n# Step 1: Calculate the total GHSL population per barrio popular using zonal statistics\n\n# Convert to the format expected by rasterstats\ngeometries = [geom for geom in renabap_pba_intersect.geometry]\n\n# Use rasterstats for vectorized zonal statistics\nstats = rasterstats.zonal_stats(\n    geometries,\n    ghsl_clipped.values[0],  # rasterstats expects 2D array\n    affine=ghsl_clipped.rio.transform(),\n    stats=[\"sum\"],\n    nodata=ghsl_clipped.rio.nodata,\n)\n\n# Extract the sum values\nghsl_totals = [stat[\"sum\"] if stat[\"sum\"] is not None else 0 for stat in stats]\n\n# Add the GHSL population estimates as a new column\nrenabap_pba_intersect[\"ghsl_pop_est\"] = ghsl_totals\n\nfrom rasterio.features import rasterize\nimport numpy as np\n\n# Get the reference raster properties from GHSL data\nreference_raster = ghsl_clipped\nreference_transform = reference_raster.rio.transform()\nreference_crs = reference_raster.rio.crs\nreference_shape = reference_raster.shape[1:]  # Get 2D shape (height, width)\n\n\n# Prepare geometries and values for rasterization\ngeometries_ghsl = [\n    (geom, value)\n    for geom, value in zip(\n        renabap_pba_intersect.geometry, renabap_pba_intersect[\"ghsl_pop_est\"]\n    )\n]\ngeometries_familias = [\n    (geom, value)\n    for geom, value in zip(\n        renabap_pba_intersect.geometry, renabap_pba_intersect[\"familias_aproximadas\"]\n    )\n]\n\n# Create GHSL population raster\nghsl_pop_raster = rasterize(\n    geometries_ghsl,\n    out_shape=reference_shape,\n    transform=reference_transform,\n    fill=0,\n    dtype=np.float32,\n    all_touched=False,\n)\n\n# Create familias aproximadas raster\nfamilias_raster = rasterize(\n    geometries_familias,\n    out_shape=reference_shape,\n    transform=reference_transform,\n    fill=0,\n    dtype=np.float32,\n    all_touched=False,\n)\n\n\n# Step 1: Divide original GHSL by the barrio-level GHSL to get fractional population\n# Use masking to avoid division on invalid cells\nmask = (ghsl_clipped.values[0] != -200) & (ghsl_pop_raster &gt; 0.1)\nghsl_fractional = np.full_like(ghsl_clipped.values[0], -200, dtype=np.float64)\nghsl_fractional[mask] = ghsl_clipped.values[0][mask] / ghsl_pop_raster[mask]\n\n# Step 2: Multiply fractional population by familias aproximadas to get downscaled data\nmask2 = (ghsl_fractional != -200) & (familias_raster &gt; 0)\nfamilias_downscaled = np.full_like(ghsl_clipped.values[0], -200, dtype=np.float64)\nfamilias_downscaled[mask2] = ghsl_fractional[mask2] * familias_raster[mask2]\n\n# Verify the results - exclude -200 from range calculations\nghsl_valid = ghsl_clipped.values[0] != -200\nfractional_valid = ghsl_fractional != -200\ndownscaled_valid = familias_downscaled != -200\n\n\n\n# Check that the sum of downscaled familias equals the original familias aproximadas\ntotal_original_familias = renabap_pba_intersect[\"familias_aproximadas\"].sum()\ntotal_downscaled_familias = np.sum(familias_downscaled[downscaled_valid])\n\n# Create GHSL tidy dataframe with matching structure\nghsl_tidy = []\n\nfor idx, row in settlement_hazard.iterrows():\n    stats = zonal_stats(\n        [row.geometry],\n        familias_downscaled,  # your numpy array\n        affine=reference_transform,  # get transform from your xarray\n        stats=[\"sum\"],\n        nodata=-200,  # use your actual nodata value\n    )[0]\n\n    ghsl_tidy.append(\n        {\n            \"id_renabap\": row[\"id_renabap\"],\n            \"peligrosidad\": row[\"PELIGROSID\"],\n            \"fam_expuestas_ghsl\": stats[\"sum\"] if stats[\"sum\"] is not None else 0,\n        }\n    )\n\nghsl_tidy = pd.DataFrame(ghsl_tidy)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>RENABAP</span>"
    ]
  },
  {
    "objectID": "renabap.html#objetivos",
    "href": "renabap.html#objetivos",
    "title": "3  RENABAP",
    "section": "3.2 Objetivos",
    "text": "3.2 Objetivos\nEl objetivo principal de este análisis es calcular con mayor precisión la exposición poblacional en la región del Partido de La Plata, comparando el enfoque actual de interpolación por área de datos del RENABAP con enfoques de mapeo dasimétrico de mayor resolución utilizando datos de la Capa Global de Asentamientos Humanos (GHSL) y el conjunto de datos de edificios abiertos de Google-Microsoft OpenStreetMap.\nPara lograr este objetivo, necesitamos:\n\nExplicar y comparar metodologías: Desarrollar una comprensión clara de las diferencias entre interpolación por área y mapeo dasimétrico\nEvaluar fuentes de datos: Analizar las ventajas y limitaciones de cada conjunto de datos utilizado\nCrear estimaciones robustas: Generar un rango de posibles exposiciones poblacionales para informar la toma de decisiones\nPriorizar recursos: Identificar áreas donde se requiera recopilación de datos más precisa",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>RENABAP</span>"
    ]
  },
  {
    "objectID": "renabap.html#contexto",
    "href": "renabap.html#contexto",
    "title": "3  RENABAP",
    "section": "3.3 Contexto",
    "text": "3.3 Contexto\n\n\nMostrar código\nimport matplotlib.pyplot as plt\nimport contextily as ctx\n\n\nfrom io import BytesIO, StringIO\nfrom owslib.wfs import WebFeatureService\nfrom shapely.geometry import box\nimport geopandas as gpd\nimport requests\nimport pandas as pd\nimport pandas as pd\nimport geopandas as gpd\nimport requests\nfrom io import StringIO\n\nimport boto3\nimport duckdb\n\n\nimport matplotlib.pyplot as plt\n\nimport numpy as np\nimport s2sphere\nfrom botocore.config import Config\nfrom rasterstats import zonal_stats\nimport matplotlib.pyplot as plt\nimport rasterstats\nimport matplotlib.pyplot as plt\nfrom scipy import stats\nimport seaborn as sns\nfrom rasterio.features import rasterize\nimport numpy as np\n\n\nimport rioxarray\nfrom shapely.geometry import box\nfrom shapely.geometry import box\n\n\nUSE_CRS = \"EPSG:5349\"\n\n\n# Generic mapping functions\ndef setup_base_map(figsize=(12, 10), bounds=None, padding_x=500, padding_y=500):\n    \"\"\"Create figure and set up basic map boundaries with padding.\"\"\"\n    if bounds is None:\n        bounds = renabap_pba_intersect.total_bounds\n\n    # Convert bounds to Web Mercator for basemap compatibility\n    if bounds is not None:\n        # Create a temporary GeoDataFrame with the bounds to reproject\n        temp_bounds = gpd.GeoDataFrame(\n            geometry=[box(bounds[0], bounds[1], bounds[2], bounds[3])], crs=USE_CRS\n        )\n        bounds_3857 = temp_bounds.to_crs(\"EPSG:3857\").total_bounds\n    else:\n        bounds_3857 = bounds\n\n    fig, ax = plt.subplots(figsize=figsize)\n    ax.set_xlim(bounds_3857[0] - padding_x, bounds_3857[2] + padding_x)\n    ax.set_ylim(bounds_3857[1] - padding_y, bounds_3857[3] + padding_y)\n    return fig, ax\n\n\ndef add_basemap(ax, zoom=13):\n    \"\"\"Add CartoDB basemap to the axes.\"\"\"\n    # The axes are already in Web Mercator from setup_base_map\n    ctx.add_basemap(\n        ax,\n        source=ctx.providers.CartoDB.PositronNoLabels,\n        zorder=0,\n        zoom=zoom,\n    )\n\n    return ax\n\n\ndef add_north_arrow(ax, x=0.95, y=0.05, arrow_length=0.04):\n    \"\"\"Add a north arrow to the map.\"\"\"\n    # Add north arrow, https://stackoverflow.com/a/58110049/604456\n    ax.annotate('N', xy=(x, y), xytext=(x, y-arrow_length),\n                arrowprops=dict(facecolor='black', width=3, headwidth=10),\n                ha='center', va='center', fontsize=14,\n                xycoords=ax.transAxes)\n\n\ndef add_la_plata_outline(ax):\n    \"\"\"Add the outline of Partido de La Plata to a map.\"\"\"\n    la_plata_3857 = la_plata.to_crs(\"EPSG:3857\")\n    la_plata_3857.plot(\n        ax=ax,\n        facecolor='none',\n        edgecolor='black',\n        linewidth=0.5,\n        linestyle='--',\n        legend=False,\n        zorder=5\n    )\n\ndef create_consistent_map(title, bounds=None):\n    \"\"\"Create a map with consistent styling and basemap.\"\"\"\n    fig, ax = setup_base_map(bounds=bounds)\n\n    # Add basemap\n    add_basemap(ax)\n\n    # Add north arrow\n    add_north_arrow(ax)\n\n    # Add La Plata partido outline\n    add_la_plata_outline(ax)\n\n    # Set title\n    ax.set_title(title, fontsize=16, fontweight=\"bold\", pad=20)\n\n    # Remove axes\n    ax.set_axis_off()\n\n    return fig, ax\n\n\n\ndef wfs_to_gdf(\n    wfs_url: str, layer_name: str, srs: str = \"EPSG:4326\"\n) -&gt; gpd.GeoDataFrame:\n    \"\"\"\n    Descarga una capa WFS y la devuelve como GeoDataFrame.\n\n    Args:\n        wfs_url (str): URL del servicio WFS.\n        layer_name (str): Nombre de la capa (typename).\n        srs (str): Código EPSG del sistema de referencia de coordenadas.\n\n    Returns:\n        gpd.GeoDataFrame: Capa descargada como GeoDataFrame.\n    \"\"\"\n    wfs = WebFeatureService(url=wfs_url, version=\"2.0.0\")\n    response = wfs.getfeature(typename=layer_name, srsname=srs)\n    gdf = gpd.read_file(BytesIO(response.read()))\n    return gdf\n\n\n\n\nMostrar código\n### import data\n\nresponse = requests.get(\n    \"https://www.argentina.gob.ar/sites/default/files/renabap-2023-12-06.geojson\"\n)\nrenabap = gpd.read_file(StringIO(response.text))\nrenabap_pba = renabap[renabap[\"provincia\"] == \"Buenos Aires\"]\nrenabap_pba = renabap_pba.to_crs(USE_CRS)\n\npeligro_path = \"/home/nissim/Documents/dev/fulbright/ciut-riesgo/notebooks/data/la_plata_pelig_2023_datos_originales.geojson\"\npeligro = gpd.read_file(peligro_path)\npeligro = peligro.to_crs(USE_CRS)\n\npeligro_bounds = peligro.total_bounds\npeligro_bbox = box(*peligro_bounds)\n\npartidos = wfs_to_gdf(\n    wfs_url=\"https://geo.arba.gov.ar/geoserver/idera/wfs\",\n    layer_name=\"idera:Departamento\",\n    srs=\"EPSG:5347\",\n)\n\npartidos = partidos.to_crs(USE_CRS)\nla_plata = partidos[partidos[\"fna\"] == \"Partido de La Plata\"]\n\n# Obtener la geometría principal\nmain_geom = la_plata.geometry.iloc[0]\n\n# Si es un MultiPolygon, mantener solo el polígono más grande (el partido principal)\n# Esto elimina la pequeña isla que aparece en los datos\nif main_geom.geom_type == 'MultiPolygon':\n    # Obtener todos los polígonos y mantener el que tenga mayor área\n    largest_polygon = max(main_geom.geoms, key=lambda p: p.area)\n    la_plata.geometry.iloc[0] = largest_polygon\n\nla_plata_bbox = la_plata.geometry.iloc[0]\n\nrenabap_pba_intersect = renabap_pba[\n    renabap_pba.geometry.intersects(la_plata_bbox)\n].copy()\n\n\n\n# Calculate total area of RENABAP settlements in hectares (POSGAR projection is in meters)\nrenabap_total_area_ha = renabap_pba_intersect.geometry.area.sum() / 10000  # Convert m² to hectares\nla_plata_area_ha = la_plata.geometry.iloc[0].area / 10000 \npercentage_coverage = (renabap_total_area_ha / la_plata_area_ha) * 100\n\n# Get common bounds for all maps\ncommon_bounds = la_plata.total_bounds\n\n# Intersect settlements with hazard zones\nsettlement_hazard = gpd.overlay(renabap_pba_intersect, peligro, how=\"intersection\")\n\n\n/tmp/ipykernel_627798/4181857673.py:34: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\nYou are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\nA typical example is when you are setting values in a column of a DataFrame, like:\n\ndf[\"col\"][row_indexer] = value\n\nUse `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n  la_plata.geometry.iloc[0] = largest_polygon\n/tmp/ipykernel_627798/4181857673.py:34: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  la_plata.geometry.iloc[0] = largest_polygon\n\n\nHay un total de python len(renabap_pba_intersect) barrios populares en el Partido de La Plata, que representan python renabap_pba_intersect['familias_aproximadas'].sum() familias. Estos barrios ocupan python renabap_total_area_ha hectáreas del Partido de La Plata, o python percentage_coverage por ciento del partido.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>RENABAP</span>"
    ]
  }
]